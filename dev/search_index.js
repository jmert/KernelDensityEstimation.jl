var documenterSearchIndex = {"docs":
[{"location":"showcase/bk18_likelihood/#CosmoMC-Weighted-Chains-(BK18-baseline-likelihood-analysis)","page":"CosmoMC Weighted Chains (BK18 baseline likelihood analysis)","title":"CosmoMC Weighted Chains (BK18 baseline likelihood analysis)","text":"The figure below is to be compared to Figure 4 from BICEP/Keck paper XIII.\n\nAfter loading the appropriate columns from the set of MCMC chains and thinning, performing the kernel density estimation is as simple as:\n\nK_r  = kde(chain_r;  weights = chain_weight, lo =  0.0,            boundary = :closedleft)\nK_Ad = kde(chain_Ad; weights = chain_weight, lo =  0.0,            boundary = :closedleft)\nK_As = kde(chain_As; weights = chain_weight, lo =  0.0,            boundary = :closedleft)\nK_βd = kde(chain_βd; weights = chain_weight, lo =  0.8, hi =  2.4)\nK_βs = kde(chain_βs; weights = chain_weight, lo = -4.5, hi = -2.0)\nK_αd = kde(chain_αd; weights = chain_weight, lo = -1.0, hi =  0.0, boundary = :closed)\nK_αs = kde(chain_αs; weights = chain_weight, lo = -1.0, hi =  0.0, boundary = :closed)\nK_ε  = kde(chain_ε;  weights = chain_weight, lo = -1.0, hi =  1.0, boundary = :closed)\n\n(Image: )\n\nNote that because this package does not support constructing 2D density estimates, the 68% and 95% confidence \"ellipses\" in the lower-left triangle of the plot has been replaced with simple 2D histograms. The hex bin sizes have been chosen using 2× the automatically-determined bandwidths of the corresponding 1D curves.\n\nMain.@showcase_source","category":"section"},{"location":"references/#References","page":"References","title":"References","text":"M. Jones and P. Foster. A simple nonnegative boundary correction method for kernel density                  estimation. Statistica Sinica, 1005–1013 (1996).\n\n\n\nA. Lewis. GetDist: a Python package for analysing Monte Carlo samples, arXiv e-prints (2019), arXiv:1910.13970.\n\n\n\nB. Hansen. Lecture Notes on Nonparametrics (2009).\n\n\n\nZ. Botev, J. Grotowski and D. Kroese. Kernel density estimation via diffusion. The Annals of Statistics 38 (2010), arXiv:1011.2602.\n\n\n\n","category":"section"},{"location":"api/#API","page":"API","title":"API","text":"Pages = [\"api.md\"]\nDepth = 2:2","category":"section"},{"location":"api/#User-Interface","page":"API","title":"User Interface","text":"","category":"section"},{"location":"api/#Advanced-User-Interface","page":"API","title":"Advanced User Interface","text":"","category":"section"},{"location":"api/#Binning-Methods","page":"API","title":"Binning Methods","text":"","category":"section"},{"location":"api/#Density-Estimation-Methods","page":"API","title":"Density Estimation Methods","text":"","category":"section"},{"location":"api/#Bandwidth-Estimators","page":"API","title":"Bandwidth Estimators","text":"","category":"section"},{"location":"api/#Interfaces","page":"API","title":"Interfaces","text":"","category":"section"},{"location":"api/#Density-Estimation-Methods-2","page":"API","title":"Density Estimation Methods","text":"","category":"section"},{"location":"api/#KernelDensityEstimation.kde","page":"API","title":"KernelDensityEstimation.kde","text":"estim = kde(data;\n            weights = nothing, method = MultiplicativeBiasKDE(),\n            lo = nothing, hi = nothing, boundary = :open, bounds = nothing,\n            bandwidth = ISJBandwidth(), bwratio = 8 nbins = nothing)\n\nCalculate a discrete kernel density estimate (KDE) estim from samples data, optionally weighted by a corresponding vector of weights.\n\nThe default method of density estimation uses the MultiplicativeBiasKDE pipeline, which includes corrections for boundary effects and peak broadening which should be an acceptable default in many cases, but a different AbstractKDEMethod can be chosen if necessary.\n\nThe interval of the density estimate can be controlled by either the set of lo, hi, and boundary keywords or the bounds keyword, where the former are conveniences for setting bounds = (lo, hi, boundary). The minimum and maximum of v are used if lo and/or hi are nothing, respectively. (See also bounds.)\n\nThe KDE is constructed by first histogramming the input v into nbins many bins with outermost bin edges spanning lo to hi. The span of the histogram may be expanded outward based on boundary condition, dictating whether the boundaries are open or closed. The bwratio parameter is used to calculate nbins when it is not given and corresponds (approximately) to the ratio of the bandwidth to the width of each histogram bin.\n\nAcceptable values of boundary are:\n\n:open or Open\n:closed or Closed\n:closedleft, :openright, ClosedLeft, or OpenRight\n:closedright, :openleft, ClosedRight, or OpenLeft\n\nThe histogram is then convolved with a Gaussian distribution with standard deviation bandwidth. The default bandwidth estimator is the Improved Sheather-Jones (ISJBandwidth) if no explicit bandwidth is given.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.MultivariateKDE","page":"API","title":"KernelDensityEstimation.MultivariateKDE","text":"MultivariateKDE{T, N, R<:Tuple{Vararg{AbstractRange,N}, V<:AbstractVector{T}} <: AbstractKDE{T, N}\n\nFields\n\naxes::R: An N-tuple of the locations (bin centers) along the axes of the density estimate. Each axis is a range type with uniform step size.\ndensity::V: An N-dimensional array with element type T of the density estimate values.\n\nSee also UnivariateKDE and BivariateKDE.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.UnivariateKDE","page":"API","title":"KernelDensityEstimation.UnivariateKDE","text":"UnivariateKDE{T, R, V} = MultivariateKDE{T, 1, Tuple{R}, V}\n\nA simplifying alias of a 1-dimensional MultivariateKDE structure.\n\nProperties\n\nThe following properties are defined to supplement the fields of the underlying MultivariateKDE struct.\n\nx: An alias for the first (and only) axis; i.e. K.x == K.axes[1]\nf: An alias for the name density (for backwards compatibility).\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.BivariateKDE","page":"API","title":"KernelDensityEstimation.BivariateKDE","text":"BivariateKDE{T, R, V} = MultivariateKDE{T, 2, R, V}\n\nA simplifying alias of a 2-dimensional MultivariateKDE structure.\n\nProperties\n\nThe following properties are defined to supplement the fields of the underlying MultivariateKDE struct.\n\nx: An alias for the first axis; i.e. K.x == K.axes[1]\ny: An alias for the first (and last) axis; i.e. K.y == K.axes[2]\nf: An alias for the name density (for consistency with UnivariateKDE)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.Boundary","page":"API","title":"KernelDensityEstimation.Boundary","text":"@enum T Closed Open ClosedLeft ClosedRight\nconst OpenLeft = ClosedRight\nconst OpenRight = ClosedLeft\n\nEnumeration to describe the desired boundary conditions of the domain of the kernel density estimate K. For some given data d  a b, the boundary conditions have the following impact:\n\nClosed: The domain K  a b is used directly as the bounds of the binning.\nOpen: The desired domain K  (- +) is effectively achieved by widening the bounds of the data by the size of the finite convolution kernel. Specifically, the binning is defined over the range a - 8σ b + 8σ where σ is the bandwidth of the Gaussian convolution kernel.\nClosedLeft: The left half-closed interval K  a +) is used as the bounds for binning by adjusting the upper limit to the range a b + 8σ. The equivalent alias OpenRight may also be used.\nClosedRight: The right half-closed interval K  (- b is used as the bounds for binning by adjusting the lower limit to the range a - 8σ b. The equivalent alias OpenLeft may also be used.\n\n\n\n\n\n","category":"module"},{"location":"api/#KernelDensityEstimation.init","page":"API","title":"KernelDensityEstimation.init","text":"data, weights, details = init(\n        method::K, data::AbstractVector{T},\n        weights::Union{Nothing,<:AbstractVector} = nothing;\n        bounds = (nothing, nothing, Open),\n        bwratio::Real = 1,\n        nbins::Union{Nothing,<:Integer} = nothing,\n        bandwidth::Union{<:Number,<:AbstractBandwidthEstimator} = ISJBandwidth(),\n        kwargs...\n    ) where {K<:AbstractKDEMethod, T}\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.AbstractBinningKDE","page":"API","title":"KernelDensityEstimation.AbstractBinningKDE","text":"AbstractBinningKDE <: AbstractKDEMethod\n\nThe abstract supertype of data binning methods which are the first step in the density estimation process. The two supported binning methods are HistogramBinning and LinearBinning.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.HistogramBinning","page":"API","title":"KernelDensityEstimation.HistogramBinning","text":"struct HistogramBinning <: AbstractBinningKDE end\n\nBase case which generates a density estimate by histogramming the data.\n\nSee also LinearBinning\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.LinearBinning","page":"API","title":"KernelDensityEstimation.LinearBinning","text":"struct LinearBinning <: AbstractBinningKDE end\n\nBase case which generates a density estimate by linear binning of the data.\n\nSee also HistogramBinning\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.BasicKDE","page":"API","title":"KernelDensityEstimation.BasicKDE","text":"BasicKDE{M<:AbstractBinningKDE} <: AbstractKDEMethod\n\nA baseline density estimation technique which convolves a binned dataset with a Gaussian kernel truncated at its 4σ bounds.\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.LinearBoundaryKDE","page":"API","title":"KernelDensityEstimation.LinearBoundaryKDE","text":"LinearBoundaryKDE{M<:AbstractBinningKDE} <: AbstractKDEMethod\n\nA method of KDE which applies the linear boundary correction of Jones and Foster [1] as described in Lewis [2] after BasicKDE density estimation. This correction primarily impacts the KDE near a closed boundary (see Boundary) and has the effect of improving any non-zero gradient at the boundary (when compared to normalization corrections which tend to leave the boundary too flat).\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.MultiplicativeBiasKDE","page":"API","title":"KernelDensityEstimation.MultiplicativeBiasKDE","text":"MulitplicativeBiasKDE{B<:AbstractBinningKDE,M<:AbstractKDEMethod} <: AbstractKDEMethod\n\nA method of KDE which applies the multiplicative bias correction described in Lewis [2]. This correction is designed to reduce the broadening of peaks inherent to kernel convolution by using a pilot KDE to flatten the distribution and run a second iteration of density estimation (since a perfectly uniform distribution cannot be broadened further).\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\nmethod::AbstractKDEMethod: The KDE method to use for the pilot and iterative density estimation. Defaults to LinearBoundaryKDE().\n\nNote that if the given method has a configurable binning type, it is ignored in favor of the explicit binning chosen.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.AbstractBandwidthEstimator","page":"API","title":"KernelDensityEstimation.AbstractBandwidthEstimator","text":"AbstractBandwidthEstimator\n\nAbstract supertype of kernel bandwidth estimation techniques.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.SilvermanBandwidth","page":"API","title":"KernelDensityEstimation.SilvermanBandwidth","text":"SilvermanBandwidth <: AbstractBandwidthEstimator\n\nEstimates the necessary bandwidth of a vector of data v using Silverman's Rule for a Gaussian smoothing kernel:\n\n    h = left(frac43n_mathrmeffright)^15 σ̂\n\nwhere n_mathrmeff is the effective number of degrees of freedom of v, and σ̂^2 is its sample variance.\n\nSee also ISJBandwidth\n\nExtended help\n\nThe sample variance and effective number of degrees of freedom are calculated using weighted statistics, where the latter is defined to be Kish's effective sample size n_mathrmeff = (sum_i w_i)^2  sum_i w_i^2 for weights w_i. For uniform weights, this reduces to the length of the vector v.\n\nReferences\n\nHansen [3]\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.ISJBandwidth","page":"API","title":"KernelDensityEstimation.ISJBandwidth","text":"ISJBandwidth <: AbstractBandwidthEstimator\n\nEstimates the necessary bandwidth of a vector of data v using the Improved Sheather-Jones (ISJ) plug-in estimator of Botev et al. [4].\n\nThis estimator is more capable of choosing an appropriate bandwidth for bimodal (and other highly non-Gaussian) distributions, but comes at the expense of greater computation time and no guarantee that the estimator converges when given very few data points.\n\nSee also SilvermanBandwidth\n\nFields\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of bandwidth estimation. Defaults to HistogramBinning().\nbwratio::Int: The relative resolution of the binned data used by the ISJ plug-in estimator — there are bwratio bins per interval of size h₀, where the intial rough initial bandwidth estimate is given by the SilvermanBandwidth estimator. Defaults to 2.\nniter::Int: The number of iterations to perform in the plug-in estimator. Defaults to 7, in accordance with Botev et. al. who state that higher orders show little benefit.\nfallback::Bool: Whether to fallback to the SilvermanBandwidth if the ISJ estimator fails to converge. If false, an exception is thrown instead.\n\nReferences\n\nBotev et al. [4]\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.bandwidth","page":"API","title":"KernelDensityEstimation.bandwidth","text":"h = bandwidth(estimator::AbstractBandwidthEstimator, data::AbstractVector{T}\n              lo::T, hi::T, boundary::Boundary.T;\n              weights::Union{Nothing, <:AbstractVector} = nothing\n              ) where {T}\n\nDetermine the appropriate bandwidth h of the data set data (optionally with corresponding weights) using chosen estimator algorithm. The bandwidth is provided the range (lo through hi) and boundary style (boundary) of the request KDE method for use in filtering and/or correctly interpreting the data, if necessary.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.AbstractKDE","page":"API","title":"KernelDensityEstimation.AbstractKDE","text":"AbstractKDE{T,N}\n\nAbstract supertype of kernel density estimates with element type T and dimensionality N.\n\nSee also UnivariateKDE\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.AbstractKDEInfo","page":"API","title":"KernelDensityEstimation.AbstractKDEInfo","text":"AbstractKDEInfo{T,N}\n\nAbstract supertype of auxiliary information used during kernel density estimation.\n\nSee also UnivariateKDEInfo\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.MultivariateKDEInfo","page":"API","title":"KernelDensityEstimation.MultivariateKDEInfo","text":"MultivariateKDEInfo{U,N} <: AbstractKDEInfo{U,N}\n\nInformation about the density estimation process, providing insight into both the entrypoint parameters and some internal state variables.\n\nExtended help\n\nType parameters\n\nU: A unitless element type compatible with the density estimate.\nN: The dimensionality of the density estimate.\n\nFields\n\nmethod::AbstractKDEMethod: The estimation method used to generate the KDE.\nbounds::Any: The bounds specification of the estimate as passed to init(), prior to making it concrete via calling bounds().\ndomain::Union{Nothing, Tuple{Vararg{Tuple{Ei,Ei,Boundary.T} where Ei,N}}}: A tuple of the concrete range and boundary conditions of the density estimate axes after calling bounds() with the value of the .bounds field before adding any requisite padding for open boundary conditions.\nbwratio::Union{Nothing, NTuple{N,U}}: The ratio between the bandwidth and the width of a histogram bin for each axis, used only when the number of bins is not explicitly provided.\nnbins::Union{Nothing,NTuple{N,Int}}: The number of requested bins along each axis. If nothing, then the number of bins is calculated using the padded domain of the density estimate, the bandwidth, and the ratio .bwratio.\nneffective::U: Kish's effective sample size of the data, which equals the number of samples for uniformly weighted data.\nbandwidth_alg::Union{Nothing,AbstractBandwidthEstimator}: Algorithm used to estimate an appropriate bandwidth, if a concrete value was not provided to the estimator, otherwise nothing.\nbandwidth::Union{Nothing,<:AbstractMatrix{U}}: The bandwidth (square root of covariance) of the convolution kernel.\nkernel::Union{Nothing,MultivariateKDE{U,N}}: The convolution kernel used to smooth the density estimate.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.UnivariateKDEInfo","page":"API","title":"KernelDensityEstimation.UnivariateKDEInfo","text":"UnivariateKDEInfo{U, D, B, K} = MultivariateKDEInfo{U, 1, D, B, K}\n\nA simplifying alias of a 1-dimensional MultivariateKDEInfo structure.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.BivariateKDEInfo","page":"API","title":"KernelDensityEstimation.BivariateKDEInfo","text":"BivariateKDEInfo{U, D, B, K} = MultivariateKDEInfo{U, 1, D, B, K}\n\nA simplifying alias of a 2-dimensional MultivariateKDEInfo structure.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.AbstractKDEMethod","page":"API","title":"KernelDensityEstimation.AbstractKDEMethod","text":"AbstractKDEMethod\n\nThe abstract supertype of all kernel density estimation methods, including the data binning process (see AbstractBinningKDE) and subsequent density estimation techniques (such as BasicKDE).\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.bounds","page":"API","title":"KernelDensityEstimation.bounds","text":"lo, hi, bc = bounds(x, spec)\n\nDetermine the appropriate interval from lo to hi with boundary condition bc given the data vector x and bounds specification spec.\n\nPackages may specialize this method on the spec argument to modify the behavior of the interval and boundary refinement for new argument types.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.estimate","page":"API","title":"KernelDensityEstimation.estimate","text":"estim, info = estimate(method::AbstractKDEMethod, data::AbstractVector, weights::Union{Nothing, AbstractVector}; kwargs...)\nestim, info = estimate(method::AbstractKDEMethod, data::AbstractKDE, info::AbstractKDEInfo; kwargs...)\n\nApply the kernel density estimation algorithm method to the given data, either in the form of a vector of data (and optionally with corresponding vector of weights) or a prior density estimate and its corresponding pipeline info (to support being part of a processing pipeline).\n\nReturns\n\nestim::AbstractKDE: The resultant kernel density estimate.\ninfo::AbstractKDEInfo: Auxiliary information describing details of the density estimation either useful or necessary for constructing a pipeline of processing steps.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.estimator_order","page":"API","title":"KernelDensityEstimation.estimator_order","text":"p = estimator_order(::Type{<:AbstractKDEMethod})\n\nThe bias scaning of the density estimator method, where a return value of p corresponds to bandwidth-dependent biases of the order mathcalO(h^2p).\n\n\n\n\n\n","category":"function"},{"location":"showcase/simple_distributions/#showcase_simple","page":"Simple distributions","title":"Simple distributions","text":"(Image: )\n\nMain.@showcase_source","category":"section"},{"location":"showcase/#Showcase","page":"Showcase","title":"Showcase","text":"","category":"section"},{"location":"showcase/#[Simple-Distributions](showcase/simple_distributions/index.md)","page":"Showcase","title":"Simple Distributions","text":"(Image: )","category":"section"},{"location":"showcase/#[CosmoMC-Weighted-Chains-(BK18-baseline-likelihood-analysis)](showcase/bk18_likelihood/index.md)","page":"Showcase","title":"CosmoMC Weighted Chains (BK18 baseline likelihood analysis)","text":"(Image: )","category":"section"},{"location":"userguide/#User-Guide","page":"User Guide","title":"User Guide","text":"Pages = [\"userguide.md\"]\nDepth = 2:2","category":"section"},{"location":"userguide/#Getting-Started","page":"User Guide","title":"Getting Started","text":"To install KernelDensityEstimation.jl, it is recommended that you use the jmert/Registry.jl package registry, which will let you install (and depend on) the package similarly to any other Julia package in the default General registry.\n\npkg> registry add https://github.com/jmert/Registry.jl\n\npkg> add KernelDensityEstimation","category":"section"},{"location":"userguide/#Simple-kernel-density-estimate","page":"User Guide","title":"Simple kernel density estimate","text":"For the following example, we'll use a small sample of Gaussian deviates:\n\nusing KernelDensityEstimation\nx = 3 .+ 0.1 .* randn(250) # x ~ Normal(3, 0.1)\nnothing  # hide\n\nThe key interface of this package is the kde function. In its simplest incantation, you provide a vector of data and it returns a kernel density object (in the form of a UnivariateKDE structure).\n\nusing KernelDensityEstimation\n\nK = kde(x)\nnothing  # hide\n\nThe density estimate f(x) is given at locations K.x (as a StepRangeLen) with density values K.f. For instance, the mean and variance of the distribution are:\n\nμ1 = step(K.x) * sum(@. K.f * K.x)\nμ2 = step(K.x) * sum(@. K.f * K.x^2)\n\n(; mean = μ1, std = sqrt(μ2 - μ1^2))\n\nwhich agree well with the known underlying parameters (mu = 3 sigma = 01).\n\nVisualizing the density estimate (see Extensions — Makie.jl), we see a fair level of consistency between the density estimate and the known underlying model.\n\n(Image: )","category":"section"},{"location":"userguide/#Densities-with-boundaries","page":"User Guide","title":"Densities with boundaries","text":"The previous example arises often and is handled well by most kernel density estimation solutions. Being a Gaussian distribution makes it particularly well behaved, but in general distributions which are unbounded and gently fade away to zero towards pminfty are relatively easy to deal with. Despite how often the Gaussian distribution is an appropriate [approximation of the] distribution, there are still many cases where various bounded distributions are expected, and ignoring the boundary conditions can lead to a very poor density estimate.\n\nTake the simple case of the uniform distribution on the interval 0 1.\n\nRandom.seed!(101)  # hide\nx = rand(5_000)\nnothing  # hide\n\nBy default, kde assumes the distribution is unbounded, and this leads to \"smearing\" the density across the known boundaries to the regions x  0 and x  1:\n\nK0 = kde(x)\nnothing  # hide\n\n(Image: )\n\nWe can inform the estimator that we expect a bounded distribution, and it will use that information to generate a more appropriate estimate. To do so, we make use of three keyword arguments in combination:\n\nlo to dictate the lower bound of the data.\nhi to dictate the upper bound of the data.\nboundary to specify the boundary condition, such as :open (unbounded), :closed (finite), and half-open intervals :closedleft/:openright and :closedright/:openright.\n\nIn this example, we know our data is bounded on the closed interval 0 1, so we can improve the density estimate by providing that information\n\nK1 = kde(x, lo = 0, hi = 1, boundary = :closed)\nnothing  # hide\n\n(Image: )\n\nNote that in addition to preventing the smearing of the density beyond the bounds of the known distribution, the density estimate with correct boundaries is also smoother than the unbounded estimate. This is because the sharp drops at x = 0 1 no longer need to be represented, so the algorithm is no longer compromising on smoothing the interior of the distribution with retaining the cut-offs.\n\nhint: Hint\nIn addition to the aforementioned triple of lo, hi, and boundary keywords, there is a single bounds keyword which can replace all three. The built-in mechanism only accepts a tuple where bounds = (lo, hi, boundary), but the additional keyword makes it possible to customize behavior for new types of arguments. For example, there is a package extension for Distributions.jl which allows using the support of a distribution to automatically infer appropriate boundary conditions and limits.See the docstring for kde (and references therein) for more information.","category":"section"},{"location":"userguide/#Densities-of-weighted-samples","page":"User Guide","title":"Densities of weighted samples","text":"In some cases, the data to be analyzed is a weighted vector of data (represented as a vector of data and a corresponding vector of weight factors). For instance, importance sampling of an MCMC chain results in non-uniform weights that then must be considered when deriving a density estimate.\n\nTake the following toy example where we have a target parameter v and nuisance parameter p that are correlated, where a uniform prior was assumed for p:\n\nRandom.seed!(200)  # hide\n# correlation coefficient and nuisance parameter\nρ, p = 0.85, randn(500)\n# correlated target parameter\nv = ρ .* p .+ sqrt(1 - ρ^2) .* randn.()\nnothing  # hide\n\nNow suppose that you have reason to update your prior on p, believing now that positive values are twice as likely as negative ones. If the method of generating v is expensive, and because the change in prior is not extreme, it may be efficient and acceptable to instead importance sample the existing values by reweighting the samples by the ratio of the priors:\n\nbeginalign*\nP_1(p) propto 1\n\nP_2(p) propto begincases\n    1  p  0 \n    2  p ge 0 \n    endcases\nendalign*\n\nP1(z) = 1.0\nP2(z) = z ≥ 0 ? 2.0 : 1.0\nweights = P2.(p) ./ P1.(p)\nnothing  # hide\n\nWe then simply provide these weights as a keyword argument in the call to kde:\n\nK1 = kde(v)\nK2 = kde(v; weights)\nnothing  # hide\n\n(Image: )\n\nAs expected, this shifts the resultant density estimate to the right, toward more positive values.\n\nnote: Note\nThe effective sample size (UnivariateKDEInfo.neffective) is calculated from the weights using Kish's definition. Both of the bandwidth estimators (SilvermanBandwidth and ISJBandwidth) use this definition in scaling the bandwidth with the (effective) sample size.","category":"section"},{"location":"explain/#Explanation","page":"Explanation","title":"Explanation","text":"Pages = [\"explain.md\"]\nDepth = 2:2","category":"section"},{"location":"explain/#Estimator-Pipeline","page":"Explanation","title":"Estimator Pipeline","text":"","category":"section"},{"location":"explain/#Direct-Comparisons","page":"Explanation","title":"Direct Comparisons","text":"The following figures provide direct comparisons of the four major steps in the estimator pipeline described above through their visual impact on a few example distributions.\n\nLinearBinning: The data is histogrammed onto a uniformly spaced grid.\nFor visualization purposes, the histograms are plotted with bins with a width equal to the automatically determined bandwidth (see Bandwidth Estimators below) for the distribution, whereas the remaining panels use 8× as many data points to achieve a smoother curve.\nBasicKDE: This step convolves with the histogram with the Gaussian kernel in order to smooth the data from a discontinuous histogram into a smooth curve.\nThe basic density estimator is sufficient for an unbounded and smooth distribution like the Normal case. In the cases of the HalfNormal and Uniform distributions that have non-zero boundaries, though, the distribution is severely underestimated near the boundaries.\nLinearBoundaryKDE: This step corrects for the boundary effects by recovering both the normalization (due to convolving with implicit zeros beyond the boundary) and recovers the slope of the distribution near boundaries.\nCompared to the BasicKDE step, the HalfNormal and Uniform distributions near their boundaries are significantly improved.\nMultiplicativeBiasKDE: The final stage permits use of a larger bandwidth to achieve a smoother density estimate without sacrificing the sharpness of any curves / peaks. The algorithm automatically increases the bandwidth when the multiplicative bias correction is used.\nThe visual impact is more subtle than in previous stages, but the smoothness of the Uniform and Chisq3 distributions compared to the previous stage are a consequence of the multiplicative bias correction permitting a larger kernel bandwidth (without broadening the peak in the Chisq3 distribution).\n\ndetails: Plotting Code\nusing CairoMakie\nusing Distributions\nusing Random\n\nimport KernelDensityEstimation as KDE\n\ndists = [\n    \"Normal\" => Normal(0.0, 1.0),\n    \"Chisq3\" => Chisq(3.0),\n    \"HalfNormal\" => truncated(Normal(0.0, 1.0); lower = 0.0),\n    \"Uniform\" => Uniform(0.0, 1.0),\n]\n\nestimators = [\n    KDE.LinearBinning(),\n    KDE.BasicKDE(),\n    KDE.LinearBoundaryKDE(),\n    KDE.MultiplicativeBiasKDE(),\n]\n\nfor (name, dist) in dists\n    fig = Figure(size = (900, 400))\n    axs = Axis[]\n\n    for (ii, method) in enumerate(estimators)\n        dohist = method isa KDE.AbstractBinningKDE\n\n        Random.seed!(123)  # hide\n        rv = rand(dist, 5_000)\n        dens = KDE.kde(rv; method, bounds = dist, bwratio = dohist ? 1 : 8)\n\n        ax = Axis(fig[1, ii]; title = string(nameof(typeof(method))),\n                              xlabel = \"value\", ylabel = \"density\")\n        lines!(ax, dist, color = (:black, 0.5), linestyle = :dash, label = \"true\")\n        plotter! = method isa KDE.AbstractBinningKDE ? stairs! : lines!\n        plotter!(ax, dens; label = \"estimate\",\n                           color = dohist ? :blue3 : :firebrick3)\n        scatter!(ax, [0], [0], color = (:black, 0.0))  # transparent dot to stop suppressed y=0 axis\n        ii > 1 && hideydecorations!(ax, grid = false, ticks = false)\n\n        push!(axs, ax)\n    end\n    linkaxes!(axs...)\n\n    Label(fig[0, :], name, font = :bold, fontsize = 20)\n    save(\"comparison_$name.svg\", fig)\nend\nnothing  # hide\n\n(Image: ) (Image: ) (Image: ) (Image: )","category":"section"},{"location":"explain/#Bandwidth-Estimators","page":"Explanation","title":"Bandwidth Estimators","text":"","category":"section"},{"location":"#Kernel-Density-Estimation","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"import Markdown\nreadmetxt = read(joinpath(dirname(@__FILE__), \"..\", \"..\", \"README.md\"), String)\nreadme = Markdown.parse(readmetxt)\n\n# Keep the contents between the title heading and the first horizontal rule (exclusive)\nii = findfirst(x -> x isa Markdown.Header{1}, readme.content)\njj = findfirst(x -> x isa Markdown.HorizontalRule, readme.content)\nreadme.content = readme.content[ii+1:jj-1]\n\nreadme","category":"section"},{"location":"#Why-another-kernel-density-estimation-package?","page":"Kernel Density Estimation","title":"Why another kernel density estimation package?","text":"As of Nov 2024, much of the Julia ecosystem uses the KernelDensity.jl package (possibly implicitly, such as through density plots in Makie.jl, StatsPlots.jl, etc).\n\nConsider the following (toy) examples: one case where we have samples drawn from a Gaussian distribution, and a second where only the positive values are retained.\n\nusing Random\nRandom.seed!(1234)  # hide\n\n# A vector of Gaussian random deviates\nrv_gauss = randn(500)\n# and its expected distribution\nx = -5.0:0.01:5.0\nexp_gauss = @. exp(-x^2 / 2) / sqrt(2π)\n\n# Then filter the random deviates to be strictly positive\nrv_trunc = filter(>(0.0), rv_gauss)\n# and its corresponding distribution (×2 to keep normalization)\nexp_trunc = @. ifelse(x > 0.0, 2exp_gauss, 0.0)\nnothing  # hide\n\ndetails: Plotting Setup\nusing CairoMakie\n\nfunction draw_KD(grid, rv, (x, exp_dist), (z, kde_dist), title)\n    ax = Axis(grid, title = title)\n    # draw the reference expectation distribution\n    lines!(ax, x, exp_dist, linestyle = :dash, color = :black)\n    # draw the kernel density estimate\n    lines!(ax, z, kde_dist, linewidth = 2, color = Cycled(1))\n\n    # add a shadow axis and marks along the bottom edge to indicate\n    # the location of the random deviates\n    ax2 = Axis(grid, limits = (nothing, (0.0, 1.0)))\n    vlines!(ax2, rv, ymin=0.0, ymax=0.03, linewidth = 0.5, color = (:black, 0.2))\n    hidedecorations!(ax2)\n    hidespines!(ax2)\n    linkxaxes!(ax2, ax)\n\n    # fix the range of the axes\n    xlims!(ax2, -5.9, 5.9)\nend\nnothing  # hide\n\nIf we then plot the outputs of running the KernelDensity.kde method on each of these two vectors:\n\nimport KernelDensity as KD\n\nkd_gauss = KD.kde(rv_gauss)\nkd_trunc = KD.kde(rv_trunc)\nnothing  # hide\n\n(Image: )\n\ndetails: Plotting Code\nfig = Figure(size = (800, 400))\n\n# Gaussian distribution & KDE\nref = (x, exp_gauss)\nkd = (kd_gauss.x, kd_gauss.density)\ndraw_KD(fig[1, 1], rv_gauss, ref, kd, \"Gaussian\")\n\n# Truncation Gaussian distribution & KDE\nref = (x, exp_trunc)\nkd = (kd_trunc.x, kd_trunc.density)\ndraw_KD(fig[1, 2], rv_trunc, ref, kd, \"Truncated Gaussian\")\n\nLabel(fig[0, :], \"KernelDensity.jl\", font = :bold, fontsize = 20)\n\nsave(\"example_kerneldensity.svg\", fig)  # hide\nnothing  # hide\n\nFor the Gaussian distribution (left) where there are no edges, the density estimate appears to be a reasonable approximation of the known Gaussian distribution. In comparison, though, the truncated Gaussian distribution (right) fails to represent the hard cut-off at x = 0, instead \"leaking\" below zero with non-zero density despite the known closed boundary.\n\nClosed boundaries are common among many probability distributions,[bounded] and therefore the need to estimate a density corresponding to a (semi-)bounded distribution arises often. This package provides a density estimator that uses any provided boundary conditions to account for edge boundary effects, reproducing a more faithful representation of the underlying distribution.\n\n[bounded]: For example, see the list of distributions with bounded and semi-infinite support on Wikipedia.\n\nRepeating the density estimation on the Gaussian and truncated Gaussian distributions shown above instead with this package's kde method:\n\nimport KernelDensityEstimation as KDE\n\nkde_gauss = KDE.kde(rv_gauss)\nkde_trunc = KDE.kde(rv_trunc, lo = 0.0, boundary = :closedleft)\nnothing  # hide\n\n(Image: )\n\ndetails: Plotting Code\nfig = Figure(size = (800, 400))\n\n# Gaussian distribution & KDE\nref = (x, exp_gauss)\nkd = (kde_gauss...,)\ndraw_KD(fig[1, 1], rv_gauss, ref, kd, \"Gaussian\")\n\n# Truncation Gaussian distribution & KDE\nref = (x, exp_trunc)\nkd = (kde_trunc...,)\ndraw_KD(fig[1, 2], rv_trunc, ref, kd, \"Truncated Gaussian\")\n\nLabel(fig[0, :], \"KernelDensityEstimation.jl\", font = :bold, fontsize = 20)\n\nsave(\"example_kerneldensityestimation.svg\", fig)  # hide\nnothing  # hide\n\nMost obviously, the truncated distribution retains its closed boundary condition at x = 0 and does not suffer from the leakage and suppression of the peak that occurs with the KernelDensity estimator. Furthermore, both density curves are smoother due to use of higher-order estimators which simultaneously permit using [relatively] wider bandwidth kernels while retaining the shapes of peaks (and non-flat slopes at closed boundaries).","category":"section"},{"location":"releasenotes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"Pages = [\"releasenotes.md\"]\nDepth = 2:2\n\n","category":"section"},{"location":"releasenotes/#v0.8.0-—-Unreleased","page":"Release Notes","title":"v0.8.0 — Unreleased","text":"The package extension for UnicodePlots.jl no longer defines the 3-argument Base.show method for the UnivariateKDE type in favor of requiring the user to explicitly plot it in the terminal with UnicodePlots.lineplot\nThe package extension for RecipesBase.jl has been added to aid in plotting within the Plots.jl ecosystem.\nVarious changes to the interfaces and type definitions have been made to allow for future support of bivariate (and possibly multivariate) density estimates.\nThe boundary function has been removed, with all of its functionality subsumed by the bounds function, which has also impacted the built-in definitions and behaviors of the bounds methods.\nThe type parameterizations of AbstractKDE and UnivariateKDE have changed in a backwards-incompatible way.\nPreviously, the univariate structure had four type parameters and subtype relationship,\nUnivariateKDE{T_input, T_density,\n              R<:AbstractRange{T_input},\n              V<:AbstractVector{T_density}\n             } <: AbstractKDE{T_input}\nwhere the first two parameters are the element types of the input data and output density estimate, respectively. The trailing two then specialize the container types of the binning and density vectors, but critically they use separate type variables in order to support unitful quantities where the units are inverses of one another. Notably, the type relationship to the abstract supertype was declared to use the input element type.\nThe new type definition is instead an alias of the more generic MultivariateKDE struct and takes the form\nUnivariateKDE{T_density,\n              R<:AbstractRange,\n              V<:AbstractVector{T_density}\n             } <: AbstractKDE{T_density}\nThe trailing two type parameters are the same, but now the output element type of the density array is used in the supertype relationship. The input element type also no longer appear as an explicit type parameter.\nThese changes are required to align with a future definition of multivariate density estimates. The explicit input element type is dropped since each axis may have different element type (i.e. units) and are implicitly available via the axis range(s). In contrast, the element type of the density array is unique, so this is the natural choice to use in the supertype relationship.\nUnivariateKDEInfo has been redefined as an alias of the more generic MultivariateKDEInfo struct.\nAn experimental MultivariateKDE type has been added to support higher dimensional density estimation. UnivariateKDE and BivariateKDE are type aliases for the 1- and 2-dimensional cases.\n\n","category":"section"},{"location":"releasenotes/#v0.7.0-—-2025-Jun-08","page":"Release Notes","title":"v0.7.0 — 2025 Jun 08","text":"A new weights keyword argument has been added to the kde function to support weighted data sets. As a consequence, the API of multiple interfaces have been changed:\nbandwidth has gained a weights keyword.\nThe init method returns three values (data, weights, and info) instead of just two.\nThe estimate function takes a mandatory weights positional argument.\nRevert the \"accurate histogram\" binning calculation made in the previous release. Further testing has shown that much of the extra work being done was ineffective, so no promise is currently made about being able to precisely bin range(lo, hi, nbins) (nor LinRange(lo, hi, len)).\n\n","category":"section"},{"location":"releasenotes/#v0.6.0-—-2024-Dec-31","page":"Release Notes","title":"v0.6.0 — 2024 Dec 31","text":"Public functions have been declared using Julia v1.11+'s public keyword.\nThe value of the bin-center for the zero-width singleton histogram has been fixed.\nImplement more accurate histogram (and linear) binning calculations. For the HistogramBinning case, it is now possible to precisely bin range(lo, hi, nbins) values into their corresponding bins (whereas previously values may be counted incorrectly one bin too low due to rounding in the floating point calculations). (Reverted in v0.7.0)\nThe implementation has been modified to support unitful quantities (without adding a new package dependency) via careful consideration and application of appropriate factors of one and/or oneunit (and relaxing type constraints or adding new type parameters to structs, where necessary). Given a vector with units u, the density object's fields (K.x, K.f) have units (u, u^-1), respectively, in correspondence with interpreting any integrated range to be a unitless probability (i.e. probability = sum(K.f[a .< K.x .< b]) * step(K.x)).\nThe package no longer depends on Roots.jl (used by the ISJ bandwidth estimator); instead, an implementation of Brent's Method is now included here directly. This not only decreases the dependence on external packages but also reduces both package load time and the precompiled package image size.\nThe documentation has generally been improved:\nA new \"Showcase\" section has been added to showcase examples of density estimation with this package.\nA User Guide has been started to give a brief introduction to installing and using the package.\nThe documentation now includes release notes.\n\n","category":"section"},{"location":"releasenotes/#v0.5.0-—-2024-Nov-21","page":"Release Notes","title":"v0.5.0 — 2024 Nov 21","text":"Have the ISJ bandwidth estimator fallback to Silverman rule automatically when it fails to converge. This is expected to happen for very flat (closed) distributions, so it's not as rare of an occurrence as originally understood.\nAdd package extension to aid in plotting with Makie.jl.\nBegin adding more extensive documentation to the package:\nAdd a README to give brief justification for the package.\nDescribe the package extensions available and what features they provide.\nDemonstrate the impact of the different stages of the estimator pipeline by comparing each stage for several example distributions.\n\n","category":"section"},{"location":"releasenotes/#v0.4.0-—-2024-Sep-09","page":"Release Notes","title":"v0.4.0 — 2024 Sep 09","text":"Add an interface method boundary which can be overloaded to implement mechanisms for automatically determining appropriate boundary conditions.\nThe two built-in methods are to convert from symbols to enum (e.g. :open to Open) and to infer the boundary conditions from a 2-tuple of finite/infinite real values.\nAdd an interface method bounds (and eponymous keyword argument to kde) which can be overloaded to implement mechanisms for automatically both the boundary condition (keyword boundary) and limits (keywords lo and hi) from an arbitrary value.\nStore more information within the UnivariateKDEInfo structure. The init uses the new fields to pass relevant parameters to later stages of the estimator pipeline.\nOn Julia v1.9+, new extension packages have been added to integrate with external packages:\nThe aforementioned boundary and bounds methods have been specialized for univariate distributions from Distributions.jl\nThe UnicodePlots.jl package, if loaded, is used to visualize the kernel density estimate at the terminal.\nIncrease the automatic bandwidth determined by the chosen bandwidth estimator for higher-order estimators (such as MultiplicativeBiasKDE) which have a lower level of bias. (See Lewis [2], §E, Eqn 35 and Footnote 10 for further details.)\nRename the boundary condition enum from Cover to Boundary.\nFix syntax or usage errors that broke compatibility with Julia v1.6.\n\nA. Lewis. GetDist: a Python package for analysing Monte Carlo samples (2019), arXiv:1910.13970.\n\n","category":"section"},{"location":"releasenotes/#v0.3.0-—-2024-Jul-21","page":"Release Notes","title":"v0.3.0 — 2024 Jul 21","text":"This release adds the ISJ bandwidth estimator described in Botev et al. [4] and uses it by default, as it is more capable of dealing both with non-Gaussian distributions and respects the complexity/nature of bounded domains.\n\nAdd an implementation of the Improved Sheather-Jones bandwidth estimator.\nRename the interface method for bandwidth estimators to bandwidth.\nConsolidate data and option pre-processing into the init method, which is the first step in the density estimation pipeline.\n\nZ. Botev, J. Grotowski and D. Kroese. Kernel density estimation via diffusion.  The Annals of Statistics 38 (2010),  arXiv:1011.2602.\n\n","category":"section"},{"location":"releasenotes/#v0.2.0-—-2024-Jul-19","page":"Release Notes","title":"v0.2.0 — 2024 Jul 19","text":"Require Julia v1.6+.\nImproved docstrings throughout.\nAdded framework for building documentation with Documenter.jl.\nMigrate specific density estimation implementations to be methods of the (new) estimate interface function (rather than overloading kde).\nFix handing of edge-case where a constant vector is given. For closed boundary conditions, the result is a zero-width singleton bin.\nFix error in widening of the KDE range based on the kernel bandwidth.\n\n","category":"section"},{"location":"releasenotes/#v0.1.0-—-2024-Jul-13","page":"Release Notes","title":"v0.1.0 — 2024 Jul 13","text":"Initial release supports:\n\nUnivariate kernel density estimation.\n2 binning methods (histogramming and linear binning) and 3 density estimation techniques (basic, with linear boundary correction, and/or with multiplicative bias correction\nSupport for distributions with (half-)closed boundary conditions.\nAutomatic bandwidth selection using Silverman's rule.","category":"section"},{"location":"extensions/#Package-Extensions","page":"Package Extensions","title":"Package Extensions","text":"Pages = [\"extensions.md\"]\nDepth = 2:2\n\nimportant: Important\nThis section describes features that are only available when using Julia v1.9 or newer.","category":"section"},{"location":"extensions/#ext-distributions","page":"Package Extensions","title":"Distributions.jl","text":"A univariate distribution from Distributions.jl can be used as a value for the bounds argument of kde, wherein the boundary conditions of the distribution will be used to automatically set appropriate values of lo, hi, and boundary.\n\nFor example, generating a density estimate for a non-negative parameter in a Markov chain Monte Carlo (MCMC) chain is often paired with a similarly non-negative prior. Instead of needing to explicitly determine and pass through the correct combination of lower and upper bounds and their boundary conditions, the prior distribution can be used instead.\n\nusing KernelDensityEstimation\nusing Distributions\nusing Random  # hide\nRandom.seed!(1234)  # hide\n\n# a non-negative constraint on a prior\nprior = truncated(Normal(0.0, 1.0), lower = 0.0)\n# proxy for an MCMC chain\nchain = rand(prior, 200)\n\n# prior-based boundary information on left is same as explicit options on right\nkde(chain; bounds = prior) == kde(chain; lo = 0.0, boundary = :closedleft)","category":"section"},{"location":"extensions/#ext-makie","page":"Package Extensions","title":"Makie.jl","text":"Plotting the UnivariateKDE object is natively supported within the Makie.jl system of packages. The density estimate is converted via the PointsBased trait and defaults to a line plot.\n\nPlotting via stairs is a special case, which correctly offsets the bin centers to the trailing bin edge (compatible with the default step = :pre behavior) and adds points to close the histogram with the x-axis.\n\nusing KernelDensityEstimation: kde, LinearBinning\nusing Random  # hide\nRandom.seed!(100)  # hide\n\n# 500 samples from a Chisq(ν=4) distribution\nrv = dropdims(sum(abs2, randn(4, 500), dims=1), dims=1)\nnothing  # hide\n\nusing CairoMakie\n\nK = kde(rv; lo = 0.0, boundary = :closedleft)\nH = kde(rv; lo = 0.0, boundary = :closedleft,\n            bwratio = 1.0, method = LinearBinning())\n\nfig = Figure(size=(800, 300))\nax1 = Axis(fig[1, 1], title=\"stairs\", ylabel = \"density\", xlabel = \"value\")\nax2 = Axis(fig[1, 2], title=\"lines\")\nlinkaxes!(ax1, ax2)\nhideydecorations!(ax2, grid = false, ticks = false)\n\nstairs!(ax1, H)\nlines!(ax2, K)\n\nsave(\"ext_makie.svg\", current_figure())  # hide\nnothing  # hide\n\n(Image: )","category":"section"},{"location":"extensions/#ext-plots","page":"Package Extensions","title":"Plots.jl","text":"Plotting the UnivariateKDE object is natively supported within the Plots.jl ecosystem of backends by defining a plot recipe for RecipesBase.jl.\n\nThe density estimate is interpreted by default as a :line series type with xlabel \"value\" and ylabel \"density\".\n\nusing KernelDensityEstimation: kde, LinearBinning\nusing Random  # hide\nRandom.seed!(100)  # hide\n\n# 500 samples from a Chisq(ν=4) distribution\nrv = dropdims(sum(abs2, randn(4, 500), dims=1), dims=1)\nnothing  # hide\n\nusing Plots\n\nK = kde(rv; lo = 0.0, boundary = :closedleft)\nH = kde(rv; lo = 0.0, boundary = :closedleft,\n            bwratio = 1.0, method = LinearBinning())\n\nplot(\n    plot(H, title = \"stairs\", seriestype = :stepmid),\n    plot(K, title = \"lines\", ylabel = nothing),\n    layout = (1, 2), size = (800, 300),\n    leftmargin = (2.5, :mm), bottommargin = (3.0, :mm),\n    link = :all, legend = false\n)\n\nsavefig(\"ext_plots.svg\")  # hide\nnothing  # hide\n\n(Image: )","category":"section"},{"location":"extensions/#ext-unicodeplots","page":"Package Extensions","title":"UnicodePlots.jl","text":"For quick, approximate visualization of a density within the terminal, an extension is provided for the UnicodePlots.jl package and extends the lineplot (and lineplot!) methods.\n\nusing KernelDensityEstimation\nusing Random  # hide\nRandom.seed!(100)  # hide\n\n# 500 samples from a Chisq(ν=4) distribution\nrv = dropdims(sum(abs2, randn(4, 500), dims=1), dims=1)\nnothing  # hide\n\nusing UnicodePlots\n\nK = kde(rv; lo = 0.0, boundary = :closedleft)\nlineplot(K)","category":"section"}]
}
