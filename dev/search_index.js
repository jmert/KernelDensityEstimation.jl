var documenterSearchIndex = {"docs":
[{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"CurrentModule = KernelDensityEstimation","category":"page"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"using CairoMakie\nupdate_theme!(size = (400, 300))","category":"page"},{"location":"howto/#How-to-Guides","page":"How-to Guides","title":"How-to Guides","text":"","category":"section"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"Pages = [\"howto.md\"]\nDepth = 3","category":"page"},{"location":"howto/#Extensions","page":"How-to Guides","title":"Extensions","text":"","category":"section"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"important: Important\nThis section describes features that are only available when using Julia v1.9 or newer.","category":"page"},{"location":"howto/#Distributions.jl","page":"How-to Guides","title":"Distributions.jl","text":"","category":"section"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"A univariate distribution from Distributions.jl can be used as a value for the bounds argument of kde, wherein the boundary conditions of the distribution will be used to automatically set appropriate values of lo, hi, and boundary.","category":"page"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"For example, generating a density estimate for a non-negative parameter in a Markov chain Monte Carlo (MCMC) chain is often paired with a similarly non-negative prior. Instead of needing to explicitly determine and pass through the correct combination of lower and upper bounds and their boundary conditions, the prior distribution can be used instead.","category":"page"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"using KernelDensityEstimation\nusing Distributions\nusing Random  # hide\nRandom.seed!(1234)  # hide\n\n# a non-negative constraint on a prior\nprior = truncated(Normal(0.0, 1.0), lower = 0.0)\n# proxy for an MCMC chain\nchain = rand(prior, 200)\n\n# prior-based boundary information on left is same as explicit options on right\nkde(chain; bounds = prior) == kde(chain; lo = 0.0, boundary = :closedleft)","category":"page"},{"location":"howto/#Makie.jl","page":"How-to Guides","title":"Makie.jl","text":"","category":"section"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"Plotting the UnivariateKDE object is natively supported within the Makie.jl system of packages. The density estimate is converted via the PointsBased trait and defaults to a line plot.","category":"page"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"Plotting via stairs is a special case, which correctly offsets the bin centers to the trailing bin edge (compatible with the default step = :pre behavior) and adds points to close the histogram with the x-axis.","category":"page"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"using KernelDensityEstimation: kde, LinearBinning\nusing Random  # hide\nRandom.seed!(100)  # hide\n\n# 500 samples from a Chisq(ν=4) distribution\nrv = dropdims(sum(abs2, randn(4, 500), dims=1), dims=1)\nnothing  # hide","category":"page"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"using CairoMakie\n\nK = kde(rv; lo = 0.0, boundary = :closedleft)\nH = kde(rv; lo = 0.0, boundary = :closedleft,\n            bwratio = 1.0, method = LinearBinning())\n\nfig = Figure(size=(800, 300))\nax1 = Axis(fig[1, 1], title=\"stairs\", ylabel = \"density\", xlabel = \"value\")\nax2 = Axis(fig[1, 2], title=\"lines\")\nlinkaxes!(ax1, ax2)\nhideydecorations!(ax2, grid = false, ticks = false)\n\nstairs!(ax1, H)\nlines!(ax2, K)\n\nsave(\"ext_makie.svg\", current_figure())  # hide\nnothing  # hide","category":"page"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"(Image: )","category":"page"},{"location":"howto/#UnicodePlots.jl","page":"How-to Guides","title":"UnicodePlots.jl","text":"","category":"section"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"For quick, approximate visualization of a density within the terminal, an extension is provided for the UnicodePlots.jl package. The three-argument Base.show method is defined to show the Unicode plot by default, so the distribution will be previewed at the REPL automatically.","category":"page"},{"location":"howto/","page":"How-to Guides","title":"How-to Guides","text":"using KernelDensityEstimation\nusing UnicodePlots\nusing Random  # hide\nRandom.seed!(100)  # hide\n\n# 500 samples from a Chisq(ν=4) distribution\nrv = dropdims(sum(abs2, randn(4, 500), dims=1), dims=1)\nK = kde(rv; lo = 0.0, boundary = :closedleft)","category":"page"},{"location":"tutorials/#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"explain/#Explanation","page":"Explanation","title":"Explanation","text":"","category":"section"},{"location":"explain/","page":"Explanation","title":"Explanation","text":"Pages = [\"explain.md\"]\nDepth = 2","category":"page"},{"location":"explain/#Estimator-Pipeline","page":"Explanation","title":"Estimator Pipeline","text":"","category":"section"},{"location":"explain/#Direct-Comparisons","page":"Explanation","title":"Direct Comparisons","text":"","category":"section"},{"location":"explain/","page":"Explanation","title":"Explanation","text":"The following figures provide direct comparisons of the four major steps in the estimator pipeline described above through their visual impact on a few example distributions.","category":"page"},{"location":"explain/","page":"Explanation","title":"Explanation","text":"LinearBinning: The data is histogrammed onto a uniformly spaced grid.\nFor visualization purposes, the histograms are plotted with bins with a width equal to the automatically determined bandwidth (see Bandwidth Estimators below) for the distribution, whereas the remaining panels use 8× as many data points to achieve a smoother curve.\nBasicKDE: This step convolves with the histogram with the Gaussian kernel in order to smooth the data from a discontinuous histogram into a smooth curve.\nThe basic density estimator is sufficient for an unbounded and smooth distribution like the Normal case. In the cases of the HalfNormal and Uniform distributions that have non-zero boundaries, though, the distribution is severely underestimated near the boundaries.\nLinearBoundaryKDE: This step corrects for the boundary effects by recovering both the normalization (due to convolving with implicit zeros beyond the boundary) and recovers the slope of the distribution near boundaries.\nCompared to the BasicKDE step, the HalfNormal and Uniform distributions near their boundaries are significantly improved.\nMultiplicativeBiasKDE: The final stage permits use of a larger bandwidth to achieve a smoother density estimate without sacrificing the sharpness of any curves / peaks. The algorithm automatically increases the bandwidth when the multiplicative bias correction is used.\nThe visual impact is more subtle than in previous stages, but the smoothness of the Uniform and Chisq3 distributions compared to the previous stage are a consequence of the multiplicative bias correction permitting a larger kernel bandwidth (without broadening the peak in the Chisq3 distribution).","category":"page"},{"location":"explain/","page":"Explanation","title":"Explanation","text":"details: Plotting Code\nusing CairoMakie\nusing Distributions\nusing Random\n\nimport KernelDensityEstimation as KDE\n\ndists = [\n    \"Normal\" => Normal(0.0, 1.0),\n    \"Chisq3\" => Chisq(3.0),\n    \"HalfNormal\" => truncated(Normal(0.0, 1.0); lower = 0.0),\n    \"Uniform\" => Uniform(0.0, 1.0),\n]\n\nestimators = [\n    KDE.LinearBinning(),\n    KDE.BasicKDE(),\n    KDE.LinearBoundaryKDE(),\n    KDE.MultiplicativeBiasKDE(),\n]\n\nfor (name, dist) in dists\n    fig = Figure(size = (900, 400))\n    axs = Axis[]\n\n    for (ii, method) in enumerate(estimators)\n        dohist = method isa KDE.AbstractBinningKDE\n\n        Random.seed!(123)  # hide\n        rv = rand(dist, 5_000)\n        dens = KDE.kde(rv; method, bounds = dist, bwratio = dohist ? 1 : 8)\n\n        ax = Axis(fig[1, ii]; title = string(nameof(typeof(method))),\n                              xlabel = \"value\", ylabel = \"density\")\n        lines!(ax, dist, color = (:black, 0.5), linestyle = :dash, label = \"true\")\n        plotter! = method isa KDE.AbstractBinningKDE ? stairs! : lines!\n        plotter!(ax, dens; label = \"estimate\",\n                           color = dohist ? :blue3 : :firebrick3)\n        scatter!(ax, [0], [0], color = (:black, 0.0))  # transparent dot to stop suppressed y=0 axis\n        ii > 1 && hideydecorations!(ax, grid = false, ticks = false)\n\n        push!(axs, ax)\n    end\n    linkaxes!(axs...)\n\n    Label(fig[0, :], name, font = :bold, fontsize = 20)\n    save(\"comparison_$name.svg\", fig)\nend\nnothing  # hide","category":"page"},{"location":"explain/","page":"Explanation","title":"Explanation","text":"(Image: ) (Image: ) (Image: ) (Image: )","category":"page"},{"location":"explain/#Bandwidth-Estimators","page":"Explanation","title":"Bandwidth Estimators","text":"","category":"section"},{"location":"explain/#References","page":"Explanation","title":"References","text":"","category":"section"},{"location":"explain/","page":"Explanation","title":"Explanation","text":"M. Jones and P. Foster. A simple nonnegative boundary correction method for kernel density                  estimation. Statistica Sinica, 1005–1013 (1996).\n\n\n\nA. Lewis. GetDist: a Python package for analysing Monte Carlo samples, arXiv e-prints (2019).\n\n\n\nB. Hansen. Lecture Notes on Nonparametrics (2009).\n\n\n\nZ. Botev, J. Grotowski and D. Kroese. Kernel density estimation via diffusion. The Annals of Statistics 38 (2010), arXiv:1011.2602.\n\n\n\n","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = KernelDensityEstimation","category":"page"},{"location":"reference/#User-Interface","page":"Reference","title":"User Interface","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"kde\nUnivariateKDE\nBoundary","category":"page"},{"location":"reference/#KernelDensityEstimation.kde","page":"Reference","title":"KernelDensityEstimation.kde","text":"estim = kde(v;\n            method = MultiplicativeBiasKDE(),\n            lo = nothing, hi = nothing, boundary = :open, bounds = nothing,\n            bandwidth = ISJBandwidth(), bwratio = 8 nbins = nothing)\n\nCalculate a discrete kernel density estimate (KDE) f(x) from samples v.\n\nThe default method of density estimation uses the MultiplicativeBiasKDE pipeline, which includes corrections for boundary effects and peak broadening which should be an acceptable default in many cases, but a different AbstractKDEMethod can be chosen if necessary.\n\nThe interval of the density estimate can be controlled by either the set of lo, hi, and boundary keywords or the bounds keyword, where the former are conveniences for setting bounds = (lo, hi, boundary). The minimum and maximum of v are used if lo and/or hi are nothing, respectively. (See also bounds.)\n\nThe KDE is constructed by first histogramming the input v into nbins many bins with outermost bin edges spanning lo to hi. The span of the histogram may be expanded outward based on boundary condition, dictating whether the boundaries are open or closed. The bwratio parameter is used to calculate nbins when it is not given and corresponds (approximately) to the ratio of the bandwidth to the width of each histogram bin.\n\nAcceptable values of boundary are:\n\n:open or Open\n:closed or Closed\n:closedleft, :openright, ClosedLeft, or OpenRight\n:closedright, :openleft, ClosedRight, or OpenLeft\n\nThe histogram is then convolved with a Gaussian distribution with standard deviation bandwidth. The default bandwidth estimator is the Improved Sheather-Jones (ISJBandwidth) if no explicit bandwidth is given.\n\n\n\n\n\n","category":"function"},{"location":"reference/#KernelDensityEstimation.UnivariateKDE","page":"Reference","title":"KernelDensityEstimation.UnivariateKDE","text":"UnivariateKDE{T,R<:AbstractRange{T},V<:AbstractVector{T}} <: AbstractKDE{T}\n\nFields\n\nx::R: The locations (bin centers) of the corresponding density estimate values.\nf::V: The density estimate values.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.Boundary","page":"Reference","title":"KernelDensityEstimation.Boundary","text":"@enum T Closed Open ClosedLeft ClosedRight\nconst OpenLeft = ClosedRight\nconst OpenRight = ClosedLeft\n\nEnumeration to describe the desired boundary conditions of the domain of the kernel density estimate K. For some given data d  a b, the boundary conditions have the following impact:\n\nClosed: The domain K  a b is used directly as the bounds of the binning.\nOpen: The desired domain K  (- +) is effectively achieved by widening the bounds of the data by the size of the finite convolution kernel. Specifically, the binning is defined over the range a - 8σ b + 8σ where σ is the bandwidth of the Gaussian convolution kernel.\nClosedLeft: The left half-closed interval K  a +) is used as the bounds for binning by adjusting the upper limit to the range a b + 8σ. The equivalent alias OpenRight may also be used.\nClosedRight: The right half-closed interval K  (- b is used as the bounds for binning by adjusting the lower limit to the range a - 8σ b. The equivalent alias OpenLeft may also be used.\n\n\n\n\n\n","category":"module"},{"location":"reference/#Advanced-User-Interface","page":"Reference","title":"Advanced User Interface","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"init","category":"page"},{"location":"reference/#KernelDensityEstimation.init","page":"Reference","title":"KernelDensityEstimation.init","text":"data, details = init(method::K, data::AbstractVector{T};\n                     lo::Union{Nothing,<:Real} = nothing,\n                     hi::Union{Nothing,<:Real} = nothing,\n                     boundary::Union{Symbol,Boundary.T} = :open,\n                     bounds = nothing,\n                     bandwidth::Union{<:Real,<:AbstractBandwidthEstimator} = ISJBandwidth(),\n                     bwratio::Real = 1,\n                     nbins::Union{Nothing,<:Integer} = nothing,\n                     kwargs...) where {K<:AbstractKDEMethod, T}\n\n\n\n\n\n","category":"function"},{"location":"reference/#Binning-Methods","page":"Reference","title":"Binning Methods","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"AbstractBinningKDE\nHistogramBinning\nLinearBinning","category":"page"},{"location":"reference/#KernelDensityEstimation.AbstractBinningKDE","page":"Reference","title":"KernelDensityEstimation.AbstractBinningKDE","text":"AbstractBinningKDE <: AbstractKDEMethod\n\nThe abstract supertype of data binning methods which are the first step in the density estimation process. The two supported binning methods are HistogramBinning and LinearBinning.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.HistogramBinning","page":"Reference","title":"KernelDensityEstimation.HistogramBinning","text":"struct HistogramBinning <: AbstractBinningKDE end\n\nBase case which generates a density estimate by histogramming the data.\n\nSee also LinearBinning\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.LinearBinning","page":"Reference","title":"KernelDensityEstimation.LinearBinning","text":"struct LinearBinning <: AbstractBinningKDE end\n\nBase case which generates a density estimate by linear binning of the data.\n\nSee also HistogramBinning\n\n\n\n\n\n","category":"type"},{"location":"reference/#Density-Estimation-Methods","page":"Reference","title":"Density Estimation Methods","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"BasicKDE\nLinearBoundaryKDE\nMultiplicativeBiasKDE","category":"page"},{"location":"reference/#KernelDensityEstimation.BasicKDE","page":"Reference","title":"KernelDensityEstimation.BasicKDE","text":"BasicKDE{M<:AbstractBinningKDE} <: AbstractKDEMethod\n\nA baseline density estimation technique which convolves a binned dataset with a Gaussian kernel truncated at its 4σ bounds.\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.LinearBoundaryKDE","page":"Reference","title":"KernelDensityEstimation.LinearBoundaryKDE","text":"LinearBoundaryKDE{M<:AbstractBinningKDE} <: AbstractKDEMethod\n\nA method of KDE which applies the linear boundary correction of Jones and Foster [1] as described in Lewis [2] after BasicKDE density estimation. This correction primarily impacts the KDE near a closed boundary (see Boundary) and has the effect of improving any non-zero gradient at the boundary (when compared to normalization corrections which tend to leave the boundary too flat).\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.MultiplicativeBiasKDE","page":"Reference","title":"KernelDensityEstimation.MultiplicativeBiasKDE","text":"MulitplicativeBiasKDE{B<:AbstractBinningKDE,M<:AbstractKDEMethod} <: AbstractKDEMethod\n\nA method of KDE which applies the multiplicative bias correction described in Lewis [2]. This correction is designed to reduce the broadening of peaks inherent to kernel convolution by using a pilot KDE to flatten the distribution and run a second iteration of density estimation (since a perfectly uniform distribution cannot be broadened further).\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\nmethod::AbstractKDEMethod: The KDE method to use for the pilot and iterative density estimation. Defaults to LinearBoundaryKDE().\n\nNote that if the given method has a configurable binning type, it is ignored in favor of the explicit binning chosen.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Bandwidth-Estimators","page":"Reference","title":"Bandwidth Estimators","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"AbstractBandwidthEstimator\nSilvermanBandwidth\nISJBandwidth\nbandwidth","category":"page"},{"location":"reference/#KernelDensityEstimation.AbstractBandwidthEstimator","page":"Reference","title":"KernelDensityEstimation.AbstractBandwidthEstimator","text":"AbstractBandwidthEstimator\n\nAbstract supertype of kernel bandwidth estimation techniques.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.SilvermanBandwidth","page":"Reference","title":"KernelDensityEstimation.SilvermanBandwidth","text":"SilvermanBandwidth <: AbstractBandwidthEstimator\n\nEstimates the necessary bandwidth of a vector of data v using Silverman's Rule for a Gaussian smoothing kernel:\n\n    h = left(frac43nright)^15 σ\n\nwhere n is the length of v and σ is its sample variance.\n\nSee also ISJBandwidth\n\nReferences\n\nHansen [3]\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.ISJBandwidth","page":"Reference","title":"KernelDensityEstimation.ISJBandwidth","text":"ISJBandwidth <: AbstractBandwidthEstimator\n\nEstimates the necessary bandwidth of a vector of data v using the Improved Sheather-Jones (ISJ) plug-in estimator of Botev et al. [4].\n\nThis estimator is more capable of choosing an appropriate bandwidth for bimodal (and other highly non-Gaussian) distributions, but comes at the expense of greater computation time and no guarantee that the estimator converges when given very few data points.\n\nSee also SilvermanBandwidth\n\nFields\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of bandwidth estimation. Defaults to HistogramBinning().\nbwratio::Int: The relative resolution of the binned data used by the ISJ plug-in estimator — there are bwratio bins per interval of size h₀, where the intial rough initial bandwidth estimate is given by the SilvermanBandwidth estimator. Defaults to 2.\nniter::Int: The number of iterations to perform in the plug-in estimator. Defaults to 7, in accordance with Botev et. al. who state that higher orders show little benefit.\nfallback::Bool: Whether to fallback to the SilvermanBandwidth if the ISJ estimator fails to converge. If false, an exception is thrown instead.\n\nReferences\n\nBotev et al. [4]\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.bandwidth","page":"Reference","title":"KernelDensityEstimation.bandwidth","text":"h = bandwidth(estimator::AbstractBandwidthEstimator, data::AbstractVector{T},\n              lo::T, hi::T, boundary::Boundary.T) where {T}\n\nDetermine the appropriate bandwidth h of the data set data using chosen estimator algorithm. The bandwidth is provided the range (lo through hi) and boundary style (boundary) of the request KDE method for use in filtering and/or correctly interpreting the data, if necessary.\n\n\n\n\n\n","category":"function"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/#Interfaces","page":"Reference","title":"Interfaces","text":"","category":"section"},{"location":"reference/#Density-Estimation-Methods-2","page":"Reference","title":"Density Estimation Methods","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"AbstractKDE\nAbstractKDEInfo\nUnivariateKDEInfo\nAbstractKDEMethod\nboundary\nbounds\nestimate\nestimator_order","category":"page"},{"location":"reference/#KernelDensityEstimation.AbstractKDE","page":"Reference","title":"KernelDensityEstimation.AbstractKDE","text":"AbstractKDE{T}\n\nAbstract supertype of kernel density estimates.\n\nSee also UnivariateKDE\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.AbstractKDEInfo","page":"Reference","title":"KernelDensityEstimation.AbstractKDEInfo","text":"AbstractKDEInfo{T}\n\nAbstract supertype of auxiliary information used during kernel density estimation.\n\nSee also UnivariateKDEInfo\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.UnivariateKDEInfo","page":"Reference","title":"KernelDensityEstimation.UnivariateKDEInfo","text":"UnivariateKDEInfo{T} <: AbstractKDEInfo{T}\n\nInformation about the density estimation process, providing insight into both the entrypoint parameters and some internal state variables.\n\nExtended help\n\nFields\n\nmethod::AbstractKDEMethod: The estimation method used to generate the KDE.\nbounds::Any: The bounds specification of the estimate as passed to init(), prior to making it concrete via calling bounds(). Defaults to nothing.\ninterval::Tuple{T,T}: The concrete interval of the density estimate after calling bounds() with the value of the .bounds field but before adding requisite padding for open boundary conditions. Defaults to (zero(T), zero(T)).\nboundary::Boundary.T: The concrete boundary condition assumed in the density estimate after calling boundary() with the value of the .bounds field. Defaults to Open.\nnpoints::Int: The number of values in the original data vector. Defaults to -1.\nbandwidth_alg::Union{Nothing,AbstractBandwidthEstimator}: Algorithm used to estimate an appropriate bandwidth, if a concrete value was not provided to the estimator, otherwise nothing. Defaults to nothing.\nbandwidth::T: The bandwidth of the convolution kernel. Defaults to zero(T).\nbwratio::T: The ratio between the bandwidth and the width of a histogram bin, used only when the number of bins .nbins is not explicitly provided. Defaults to one(T).\nlo::T: The lower edge of the first bin in the density estimate, after possibly adjusting for an open boundary condition compared to the .interval field. Defaults to zero(T).\nhi::T: The upper edge of the last bin in the density estimate, after possibly adjusting for an open boundary condition compared to the .interval field. Defaults to zero(T).\nnbins::Int: The number of bins used in the histogram at the beinning of the density estimatation. Defaults to -1.\nkernel::Union{Nothing,UnivariateKDE{T}}: The convolution kernel used to process the density estimate. Defaults to nothing.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.AbstractKDEMethod","page":"Reference","title":"KernelDensityEstimation.AbstractKDEMethod","text":"AbstractKDEMethod\n\nThe abstract supertype of all kernel density estimation methods, including the data binning process (see AbstractBinningKDE) and subsequent density estimation techniques (such as BasicKDE).\n\n\n\n\n\n","category":"type"},{"location":"reference/#KernelDensityEstimation.boundary","page":"Reference","title":"KernelDensityEstimation.boundary","text":"B = boundary(spec)\n\nConvert the specification spec to a boundary style B.\n\nPackages may specialize this method on the spec argument to modify the behavior of the boundary inference for new argument types.\n\n\n\n\n\n","category":"function"},{"location":"reference/#KernelDensityEstimation.bounds","page":"Reference","title":"KernelDensityEstimation.bounds","text":"lo, hi, boundary = bounds(data::AbstractVector{T}, spec) where {T}\n\nDetermine the appropriate interval, from lo to hi with boundary style boundary, for the density estimate, given the data vector data and KDE argument bounds.\n\nPackages may specialize this method on the spec argument to modify the behavior of the interval and boundary refinement for new argument types.\n\n\n\n\n\n","category":"function"},{"location":"reference/#KernelDensityEstimation.estimate","page":"Reference","title":"KernelDensityEstimation.estimate","text":"estim, info = estimate(method::AbstractKDEMethod, data::AbstractVector; kwargs...)\nestim, info = estimate(method::AbstractKDEMethod, data::AbstractKDE, info::AbstractKDEInfo; kwargs...)\n\nApply the kernel density estimation algorithm method to the given data, either in the form of a vector of data or a prior density estimate and its corresponding pipeline info (to support being part of a processing pipeline).\n\nReturns\n\nestim::AbstractKDE: The resultant kernel density estimate.\ninfo::AbstractKDEInfo: Auxiliary information describing details of the density estimation either useful or necessary for constructing a pipeline of processing steps.\n\n\n\n\n\n","category":"function"},{"location":"reference/#KernelDensityEstimation.estimator_order","page":"Reference","title":"KernelDensityEstimation.estimator_order","text":"p = estimator_order(::Type{<:AbstractKDEMethod})\n\nThe bias scaning of the density estimator method, where a return value of p corresponds to bandwidth-dependent biases of the order mathcalO(h^2p).\n\n\n\n\n\n","category":"function"},{"location":"#Kernel-Density-Estimation","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"","category":"section"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"import Markdown\nreadmetxt = read(joinpath(dirname(@__FILE__), \"..\", \"..\", \"README.md\"), String)\n# Documenter.jl needs the title in this file, so strip away the heading from the README\nreadme = Markdown.parse(readmetxt)\npopfirst!(readme.content)\nreadme","category":"page"},{"location":"#Why-another-kernel-density-estimation-package?","page":"Kernel Density Estimation","title":"Why another kernel density estimation package?","text":"","category":"section"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"As of Nov 2024, much of the Julia ecosystem uses the KernelDensity.jl package (possibly implicitly, such as through density plots in Makie.jl, StatsPlots.jl, etc).","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"...","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"using Random\nRandom.seed!(1234)  # hide\n\n# A vector of Gaussian random deviates\nrv_gauss = randn(500)\n# and its expected distribution\nx = -5.0:0.01:5.0\nexp_gauss = @. exp(-x^2 / 2) / sqrt(2π)\n\n# Then filter the random deviates to be strictly positive\nrv_trunc = filter(>(0.0), rv_gauss)\n# and its corresponding distribution (×2 to keep normalization)\nexp_trunc = @. ifelse(x > 0.0, 2exp_gauss, 0.0)\nnothing  # hide","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"details: Plotting Setup\nusing CairoMakie\n\nfunction draw_KD(grid, rv, (x, exp_dist), (z, kde_dist), title)\n    ax = Axis(grid, title = title)\n    # draw the reference expectation distribution\n    lines!(ax, x, exp_dist, linestyle = :dash, color = :black)\n    # draw the kernel density estimate\n    lines!(ax, z, kde_dist, linewidth = 2, color = Cycled(1))\n\n    # add a shadow axis and marks along the bottom edge to indicate\n    # the location of the random deviates\n    ax2 = Axis(grid, limits = (nothing, (0.0, 1.0)))\n    vlines!(ax2, rv, ymin=0.0, ymax=0.03, linewidth = 0.5, color = (:black, 0.2))\n    hidedecorations!(ax2)\n    hidespines!(ax2)\n    linkxaxes!(ax2, ax)\n\n    # fix the range of the axes\n    xlims!(ax2, -5.9, 5.9)\nend\nnothing  # hide","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"If we then plot the outputs of running the KernelDensity.kde method on each of these two vectors:","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"import KernelDensity as KD\n\nkd_gauss = KD.kde(rv_gauss)\nkd_trunc = KD.kde(rv_trunc)\nnothing  # hide","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"(Image: )","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"details: Plotting Code\nfig = Figure(size = (800, 400))\n\n# Gaussian distribution & KDE\nref = (x, exp_gauss)\nkd = (kd_gauss.x, kd_gauss.density)\ndraw_KD(fig[1, 1], rv_gauss, ref, kd, \"Gaussian\")\n\n# Truncation Gaussian distribution & KDE\nref = (x, exp_trunc)\nkd = (kd_trunc.x, kd_trunc.density)\ndraw_KD(fig[1, 2], rv_trunc, ref, kd, \"Truncated Gaussian\")\n\nLabel(fig[0, :], \"KernelDensity.jl\", font = :bold, fontsize = 20)\n\nsave(\"example_kerneldensity.svg\", fig)  # hide\nnothing  # hide","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"For the Gaussian distribution (left) where there are no edges, the density estimate appears to be a reasonable approximation of the known Gaussian distribution. In comparison, though, the truncated Gaussian distribution (right) fails to represent the hard cut-off at x = 0, instead \"leaking\" below zero with non-zero density despite the known closed boundary.","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"Closed boundaries are common among many probability distributions,[bounded] and therefore the need to estimate a density corresponding to a (semi-)bounded distribution arises often. This package provides a density estimator that uses any provided boundary conditions to account for edge boundary effects, reproducing a more faithful representation of the underlying distribution.","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"[bounded]: For example, see the list of distributions with bounded and semi-infinite support on Wikipedia.","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"Repeating the density estimation on the Gaussian and truncated Gaussian distributions shown above instead with this package's kde method:","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"import KernelDensityEstimation as KDE\n\nkde_gauss = KDE.kde(rv_gauss)\nkde_trunc = KDE.kde(rv_trunc, lo = 0.0, boundary = :closedleft)\nnothing  # hide","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"(Image: )","category":"page"},{"location":"","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"details: Plotting Code\nfig = Figure(size = (800, 400))\n\n# Gaussian distribution & KDE\nref = (x, exp_gauss)\nkd = (kde_gauss...,)\ndraw_KD(fig[1, 1], rv_gauss, ref, kd, \"Gaussian\")\n\n# Truncation Gaussian distribution & KDE\nref = (x, exp_trunc)\nkd = (kde_trunc...,)\ndraw_KD(fig[1, 2], rv_trunc, ref, kd, \"Truncated Gaussian\")\n\nLabel(fig[0, :], \"KernelDensityEstimation.jl\", font = :bold, fontsize = 20)\n\nsave(\"example_kerneldensityestimation.svg\", fig)  # hide\nnothing  # hide","category":"page"}]
}
