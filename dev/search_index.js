var documenterSearchIndex = {"docs":
[{"location":"devdocs/gaussian_trunc/#Gaussian-Truncation","page":"Gaussian Truncation","title":"Gaussian Truncation","text":"Pages = [\"gaussian_trunc.md\"]\nDepth = 2:2\n\ndetails: Setup code\nusing CairoMakie\n\nusing Distributions\nusing LinearAlgebra\n\nfunction gauss1(x; Ïƒ)\n    ð’Ÿ = Normal(zero(Ïƒ), Ïƒ)\n    return pdf.(ð’Ÿ, x)\nend\n\nfunction gauss2(x, y; Î£)\n    ð’Ÿ = MvNormal(fill!(similar(Î£, 2), 0), Î£)\n    xy = Iterators.product(x, y)\n    return map(Iterators.product(x, y)) do xy\n        pdf(ð’Ÿ, [xy...])\n    end\nend\nnothing  # hide","category":"section"},{"location":"devdocs/gaussian_trunc/#Univariate-(1D)","page":"Gaussian Truncation","title":"Univariate (1D)","text":"","category":"section"},{"location":"devdocs/gaussian_trunc/#Definition","page":"Gaussian Truncation","title":"Definition","text":"Given the variance Ïƒ^2, the 1D Gaussian distribution is defined for x  â„ to be\n\nbeginalign\n    G(x Ïƒ)  frac1sqrt2Ï€Ïƒ^2 e^-x^2  2Ïƒ^2\n    labeleqngaussian1\nendalign\n\nwhere we assume the distribution has mean equal to zero (Î¼ = 0).\n\nSince the Gaussian function is defined over the entire real line, we must truncate it to a finite size before convolution. We choose to truncate at the 4Ïƒ bounds, which simply means that the domain is restricted to x  -4Ïƒ +4Ïƒ.\n\nWe choose Ïƒ = h (the bandwidth) and guarantee an odd, whole number of points in the discrete kernel by letting\n\nbeginalign\n    x_i = -n_h Delta x + i Delta x  textfor  i  0 1  2n_h\nendalign\n\nwhere\n\n    n_h = leftlceil frac4hDelta x rightrceil\n\nand Delta x is the bin size of the histogrammed density.","category":"section"},{"location":"devdocs/gaussian_trunc/#Multivariate-(2D-and-ND)","page":"Gaussian Truncation","title":"Multivariate (2D & ND)","text":"","category":"section"},{"location":"devdocs/gaussian_trunc/#Definition-2","page":"Gaussian Truncation","title":"Definition","text":"Given a covariance matrix ðšº, the multivariate Gaussian distribution is defined for ð’™  â„^n to be\n\nbeginalign\n    G(ð’™ ðšº)  frac1sqrt(2Ï€)^n ðšº e^-ð’™^ ðšº^-1 ð’™  2\n    labeleqngaussian\nendalign\n\nwhere we assume the distribution has mean equal to zero (ð = ðŸŽ) and ðšº = det ðšº. Like the univariate case, the multivariate Gaussian is unbounded in all directions and must be truncated to a finite range to be of practical use. We extend the univariate truncation definition that x  -4Ïƒ +4Ïƒ in the following way.\n\nWe start with the definition from 1D and note that as long as ðšº is diagonal, the marginalized density over dimensions is equivalent (up to normalization) to the corresponding 1D Gaussian, as shown in Figure 1.\n\n# Define (co)variances in x, y dimensions\nÏƒ_x, Ïƒ_y = 2.0, 1.0\nÎ£ = Diagonal([Ïƒ_x^2, Ïƒ_y^2])\n\n# Construct the 2D Gaussian distribution\nxx = range(-10, 10, step = 0.1)\nyy = range(-6, 6, step = 0.1)\nkern = gauss2(xx, yy; Î£)\n# and the corresponding 1D marginal distributions\nkern_x = gauss1(xx; Ïƒ = Ïƒ_x)\nkern_y = gauss1(yy; Ïƒ = Ïƒ_y)\nnothing  # hide\n\nGiven this equivalence, we choose to define the 4Ïƒ boundary in the multidimensional sense to be the contour level which results in the same marginalized truncation for an uncorrelated covariance matrix.\n\n# transform a 4Ïƒ circle into ellipse using the Cholesky decomposition\nÏ = 4.0  # 4Ïƒ level\nL = cholesky(Î£).L\ncontour_4Ïƒ = mapreduce(hcat, range(0, 2Ï€, 250)) do Î¸\n    s, c = Ï .* sincos(Î¸)\n    return L * [c,s]\nend\nnothing  # hide\n\n(Image: )\nA 2D Gaussian with diagonal covariance matrix (bottom-left) and the two marginal densities (solid blue) that result from summing [and renormalizing] along each axis (top and right), along with the corresponding 1D Gaussian (dashed yellow) to highlight the equivalence. The 1D 4Ïƒ points are indicated (dashed red) in the marginal densities and extended across the 2D density where they contain the 2D ellipse (solid red) that we use to define the 4Ïƒ contour in higher dimensions.\n\n\n\ndetails: Plotting Code\n# 1D Gaussians from marginalizing 2D Gaussian\nm2_x = sum(kern, dims = 2)[:] .* step(yy)\nm2_y = sum(kern, dims = 1)[:] .* step(xx)\n\n## Plotting\nfig = Figure(size = (600, 600))\n\n# 2D density\nax = Axis(fig[2, 1])\nheatmap!(ax, xx, yy, kern, colormap = Reverse(:grays), rasterize = true)\n\n# 1D marginalized density along x-axis\naxx = Axis(fig[1, 1], xautolimitmargin = (0.0, 0.0), yautolimitmargin = (0.05, 0.05))\nlines!(axx, xx, m2_x, color = Cycled(1), label = \"2D Marginal\", linewidth = 2)\nlines!(axx, xx, kern_x, color = Cycled(2), label = \"1D\", linestyle = :dash)\nhidexdecorations!(axx, grid = false, ticks = false)\n\n# 1D marginalized density along y-axis, rotated to project through 2D density\naxy = Axis(fig[2, 2], xautolimitmargin = (0.05, 0.05), yautolimitmargin = (0.0, 0.0))\nlines!(axy, m2_y, yy, color = Cycled(1), linewidth = 2)\nlines!(axy, kern_y, yy, color = Cycled(2), linestyle = :dash)\nhideydecorations!(axy, grid = false, ticks = false)\n\n# add 4Ïƒ contour and corresponding bounding box to the 2D density plot\nlines!(ax, contour_4Ïƒ, color = (:firebrick3, 0.5), label = \"4Ïƒ contour\")\nkws = (; color = (:firebrick3, 0.5), linestyle = :dash, depth_shift = -1)\nvlines!(ax,  4Ïƒ_x .* [-1, 1]; kws...)\nvlines!(axx, 4Ïƒ_x .* [-1, 1]; kws...)\nhlines!(ax,  4Ïƒ_y .* [-1, 1]; kws...)\nhlines!(axy, 4Ïƒ_y .* [-1, 1]; kws...)\n\n# construct a shared legend across all three axes\nl1 = Makie.get_labeled_plots(axx, merge = true, unique = true)\nl2 = Makie.get_labeled_plots(ax,  merge = true, unique = true)\nlplt = vcat(l1[1], l2[1])\nllbl = vcat(l1[2], l2[2])\nleg = Legend(fig[1, 2], lplt, llbl; tellwidth = false, framevisible = false)\n\n# link axes\nlinkxaxes!(ax, axx)\nlinkyaxes!(ax, axy)\nmap!(identity, axx.xticks, ax.xticks)  # force equivalence of ticks across linked axes\nmap!(identity, axy.xticks, ax.yticks)\nax.xticks = -8:4:8\n# adjust gaps to make more compact\ncolgap!(fig.layout, Fixed(4))\nrowgap!(fig.layout, Fixed(4))\ncolsize!(fig.layout, 1, Relative(0.8))  # 80% of space to 2D plot\nrowsize!(fig.layout, 2, Aspect(1, length(yy) / length(xx)))  # square data units\nrowsize!(fig.layout, 1, Aspect(2, 1))  # marginal \"heights\" are equal\nresize_to_layout!(fig)\n\nsave(\"gaussian_trunc_2dmarginal.svg\", fig, backend = CairoMakie)  # hide\nnothing  # hide\n\nTherefore, our goal is to obtain the [hyper]rectangle which contains the 4Ïƒ contour, as shown in Figure 2, for any arbitrary covariance.\n\nA non-diagonal covariance may be diagonalized via the eigenvalue decomposition, so we can do the reverse to construct a non-diagonal covaraince matrix where we know the 4Ïƒ ellipse a priori.\n\nLet ð‘¹ be a rotation matrix, and let ðšº be the non-diagonal covariance constructed by applying the rotation to our diagonal covariance.\n\n    ðšº = ð‘¹ ðšº ð‘¹^\n\nÎ¸ = 22.5  # arbitrary angle, for demonstration\nR = [\n    cosd(Î¸) -sind(Î¸);\n    sind(Î¸)  cosd(Î¸)\n]\nÎ£â€² = R * Î£ * R'\nLâ€² = cholesky(Î£â€²).L\nkernâ€² = gauss2(xx, yy; Î£ = Î£â€²)\ncontour_4Ïƒâ€² = R * contour_4Ïƒ\nnothing  # hide\n\nOur problem is now:\n\nGiven the arbitrary covariance matrix ðšº, what is the bounding box ð–¡  â„^n = bigotimes_i -v_i +v_i which contains the 4Ïƒ contour?\n\nOne might be tempted to expect that the maximum coordinate value over all rotated, scaled versions of the Cartesian unit vectors ðž_j â€” i.e. the principle axes of the ellipse â€” would be the answer.\n\n    ð’ƒ_i  max_j left( ÏÏƒ_j left ð‘¹  ðž_j right right)_i\n\nFigure 2 visually demonstrates a counterexample where that this is not the case, though.\n\n(Image: )\nGaussian distribution with non-diagonal covariance along with a direct rotation of the 4Ïƒ contour (solid red). Reducing over the principle axes of the ellipse (solid green and cyan) defines a bounding box (dashed blue) which is smaller than the box that contains the contour (dashed red).\n\n\n\ndetails: Plotting Code\nfig = Figure(size = (500, 500))\n\n# 2D density and directly-rotated 4Ïƒ contour\nax = Axis(fig[1, 1])\nheatmap!(ax, xx, yy, kernâ€², colormap = Reverse(:grays), rasterize = true)\nlines!(ax, contour_4Ïƒâ€², color = :firebrick3, label = \"4Ïƒ contour\")\n\n# directly-rotated 4Ïƒ contour and corresponding empirical bounding box\nBx = [extrema(contour_4Ïƒâ€²[1, :])...]\nBy = [extrema(contour_4Ïƒâ€²[2, :])...]\nvlines!(ax, Bx, color = (:firebrick3, 0.8), linestyle = :dash)\nhlines!(ax, By, color = (:firebrick3, 0.8), linestyle = :dash)\n\n# principle axes of the ellipse...\nzz = [0.0, 0.0]\nv1 = Ï * Ïƒ_x .* R * [1, 0]\nv2 = Ï * Ïƒ_y .* R * [0, 1]\narrows2d!(ax, Point2(zz), Point2(v1), argmode = :endpoint, color = :green3, label = \"ðžâ‚\")\narrows2d!(ax, Point2(zz), Point2(v2), argmode = :endpoint, color = :cyan3, label = \"ðžâ‚‚\")\n# and corresponding bounding box\nvb = maximum([v1 v2], dims = 2)[:]\npoly!(ax, Rect(-abs.(vb), 2 .* vb), color = :transparent, strokecolor = :blue3,\n      strokewidth = 1.5, linestyle = :dash)\n\nrowsize!(fig.layout, 1, Aspect(1, length(yy) / length(xx)))\nresize_to_layout!(fig)\n\nsave(\"gaussian_trunc_eigenvec.svg\", fig, backend = CairoMakie)  # hide\nnothing  # hide\n\nThe correct procedure is to directly solve the desired maximization problem. The condition for finding the maximum value over the contour line in dimension i can be expressed as maximizing the inner product\n\nbeginalign max_ð’— ð’—^top ðž_i endalign\n\nsubject to the constraint\n\nbeginalign ð’—^top ðšº^-1 ð’— = Ï^2 endalign\n\nwhere Ï is the Ïƒ-level contour to be found (i.e. Ï = 4), ð’— is an arbitrary vector, and ðž_i is the Cartesian basis vector pointing along the i-th dimension's axis. Solving with the method of Lagrange multipliers:\n\nbeginalign\n    0 = fracddð’— left ð’—^top ðž_i - Î» left( ð’—^top ðšº^-1 ð’— - Ï^2 right) right\n     \n        nonumber\n    \n    0 = ðž_i - 2Î» ðšº^-1 ð’—\n     \n        nonumber\n    \n    ð’— = frac12Î» ðšº ðž_i\n    \n    Ï^2 = left( frac12Î» ðšº ðž_i right)^ ðšº^-1 left( frac12Î» ðšº ðž_i right)\n        nonumber\n    \n    \n    \n    Ï^2 = frac14Î»^2 ðž_i^ ðšº ðž_i\n        nonumber\n    \n    ðž_i^ ð’— = frac12Î» ðž_i^ ðšº ðž_i\n    \n    frac12Î» = fracÏsqrtðšº_ii\n        nonumber\n    \n    ðž_i^ ð’— = Ï sqrtðšº_ii\n        labeleqnmvgauss_sigmas_cov\nendalign\n\nTherefore, the bounding box that contains an arbitrary multivariate Gaussian at the Ï-sigma level is simply a box whose edges lie at Ïsqrtðšº_ii in the i-th dimension.\n\nBecause the Gaussian is defined in terms of its inverse covariance matrix, the implementation does not use the covariance matrix ðšº directly, instead making use of its Cholesky decomposition:\n\nbeginalign ð‘³ð‘³^  ðšº endalign\n\nwhere ð‘³ is a lower-triangular matrix. (See bandwidth.) To avoid reconstructing the covariance matrix from its Cholesky factors, we return to Eqn. refeqnmvgauss_sigmas_cov and replace the covariance with its decomposition and simplify:\n\nbeginalign\n    ðž_i^ ð’— = Ï sqrtðšº_ii nonumber \n    ðž_i^ ð’— = Ï sqrtðž_i^ left(ð‘³ð‘³^right) ðž_i nonumber \n    ðž_i^ ð’— = Ï sqrt(ð‘³^ ðž_i)^(ð‘³^ ðž_i) nonumber \n    ðž_i^ ð’— = Ï sqrt(ð’_i)^ ð’_i\n        qquadtextwhere quad (ð’_i)_n = ð‘³_ni text (the i-th row of ð‘³) nonumber \n    ðž_i^ ð’— = Ï ð’_i\nendalign\n\nWe find that the square root of the i-th diagonal of the covariance can be equivalently obtained from the norm of the i-th row of its Cholesky decomposition.\n\nbbox_4Ïƒ = dropdims(mapslices(l -> Ï * sqrt(l'l), Lâ€², dims = 2), dims = 2)\nnothing  # hide\n\n(Image: )\nThe same Gaussian distribution and 4Ïƒ contour (solid red) as shown in Figure 2 and the target bounding box that contains it (dashed red). The directly calculated bounding box (solid blue) derived from the Cholesky decomposition of the covariance matrix matches the empirical bounding box.\n\n\n\ndetails: Plotting Code\nfig = Figure(size = (500, 500))\n\n# 2D density and directly-rotated 4Ïƒ contour\nax = Axis(fig[1, 1])\nheatmap!(ax, xx, yy, kernâ€², colormap = Reverse(:grays), rasterize = true)\nlines!(ax, contour_4Ïƒâ€², color = :firebrick3, label = \"4Ïƒ contour\")\n\n# directly-rotated 4Ïƒ contour and corresponding empirical bounding box\nBx = [extrema(contour_4Ïƒâ€²[1, :])...]\nBy = [extrema(contour_4Ïƒâ€²[2, :])...]\nvlines!(ax, Bx, color = (:firebrick3, 0.8), linestyle = :dash)\nhlines!(ax, By, color = (:firebrick3, 0.8), linestyle = :dash)\n\n# bounding box computed from the [Cholesky decomposition of the] covariance matrix\npoly!(ax, Rect(-abs.(bbox_4Ïƒ), 2 .* bbox_4Ïƒ), color = :transparent, strokecolor = :blue3,\n      strokewidth = 1.5)\n\nrowsize!(fig.layout, 1, Aspect(1, length(yy) / length(xx)))\nresize_to_layout!(fig)\n\nsave(\"gaussian_trunc_cholnorm.svg\", fig, backend = CairoMakie)  # hide\nnothing  # hide","category":"section"},{"location":"devdocs/#Developer-Documentation","page":"Developer Documentation","title":"Developer Documentation","text":"This section provides supplemental materials that are only relevant to developers of the package. For example, we document mathematical derivations or provide justification for some of the implementation details of the various algorithms used. A typical user of the package's public interface should be able to skip this section.\n\nGaussian Truncation\nLinear Boundary Correction","category":"section"},{"location":"devdocs/#References","page":"Developer Documentation","title":"References","text":"M.Â Jones and P.Â Foster. A simple nonnegative boundary correction method for kernel density                  estimation. StatisticaÂ Sinica, 1005â€“1013 (1996).\n\n\n\nA.Â Lewis. GetDist: a Python package for analysing Monte Carlo samples, arXivÂ e-prints (2019), arXiv:1910.13970.\n\n\n\nB.Â Hansen. Lecture Notes on Nonparametrics (2009).\n\n\n\nZ.Â Botev, J.Â Grotowski and D.Â Kroese. Kernel density estimation via diffusion. TheÂ AnnalsÂ ofÂ Statistics 38 (2010), arXiv:1011.2602.\n\n\n\n","category":"section"},{"location":"devdocs/linear_boundary/#Linear-Boundary-Correction","page":"Linear Boundary Correction","title":"Linear Boundary Correction","text":"When the domain for the density estimation contains a closed edge, the boundary creates distortions in the density as the kernel \"falls off\" the edge. It is simple to correct for the normalization error, but that only improves the lowest-order bias. Lewis [2] Sec. III.A provides the so-called linear boundary kernel correction â€” given a basic density estimate f(ð’™) (the Gaussian-convolved histogram), the corrected density hat f(ð’™) is defined to be\n\n    hat f(ð’™) = A_0 f(ð’™) + sum_i A_i f_i^(1)(ð’™)\n\nwhere i ranges over each dimension in ð’™  â„^n and f_i^(1) = fracx_i f(ð’™) is the first derivative with respect to each coordinate direction.\n\nIn the following sections, the W variables are various convolutions of the domain indicator function â€” see Lewis [2] for more details.","category":"section"},{"location":"devdocs/linear_boundary/#Univariate","page":"Linear Boundary Correction","title":"Univariate","text":"Lewis [2] derives the coefficients A_0 and A_1 in general, with Eqn. 13 providing the solutions for the univariate case when i = j = 1:\n\nbeginalign*\n    A_0 = frac1W_0 - left(W_1right)^2 left(W_2right)^-1\n    \n    A_1 = -left(W_2right)^-1 W_1 A_0\nendalign*\n\nor combining with the corrected density above and rearranging terms:\n\nbeginalign*\n    hat f(x) = fracW_2 f(x) - W_1 f^(1)(x)\n        W_0 W_2 - left(W_1right)^2\nendalign*","category":"section"},{"location":"devdocs/linear_boundary/#Bivariate","page":"Linear Boundary Correction","title":"Bivariate","text":"For higher dimensions, it was easier to step back to Eqn. 12 which provides the constraints written in an implicit summation style:\n\n    langle hat f(x) rangle = left A_0 W_0 + A_1^i W_1^i right f(x)\n        - left A_0 W_1^i + A_1^j W_2^ij right f_i^(1)(x)\n\nFor 2D densities where (ij)  12  12 over two dimensions, there are three terms and therefore three conditions to be satisfied. The first term on the right is a single expression with 3 inner terms, while the second expands to two equations with three inner terms each. The condition is that the first condition evaluates to 1, whereas the latter two should evaluate to 0.\n\nbegineqnarray*\n    A_0 W_0   + A_1^1 W_1^1    + A_1^2 W_1^2    = 1 \n    A_0 W_1^1 + A_1^1 W_2^11 + A_1^2 W_2^12 = 0 \n    A_0 W_1^2 + A_1^1 W_2^12 + A_1^2 W_2^22 = 0 \nendeqnarray*\n\nThis is maybe more easily recognizable as a linear system of equations by rewriting it in matrix form.\n\n    beginbmatrix\n    W_0    W_1^1     W_1^2    \n    W_1^1  W_2^11  W_2^12 \n    W_1^2  W_2^12  W_2^22 \n    endbmatrix\n    beginbmatrix A_0  A_1^1  A_1^2 endbmatrix\n    =\n    beginbmatrix 1  0  0 endbmatrix\n\nSolving this by hand is tedious, but we can turn to any computer algebra system to solve it symbolically. In the following example, we use Julia's Symbolics.jl package to derive the set of expressions.\n\nusing Latexify\nusing LinearAlgebra\nusing Symbolics\n\nA, Wi = @variables A[0:2] W[0:2]\nWij, = @variables W[1:2, 1:2]\nWij = Symmetric(Wij)\n\n# construct the system of equations\nW = [transpose(Wi);\n     Wi[1:2]  Wij ]\neqns = Symbolics.scalarize(W * A ~ [1, 0, 0])\n\n# solve the system of equations symbolically\nsoln = symbolic_linear_solve(eqns, A, simplify = true)\n\n# and for better printing, extract the common denominator and rewrite\n# the expressions a bit\nD, = @variables D\ndenom = denominator(soln[1])\nsoln = simplify.(substitute(soln, Dict(denom => D, -denom => -D)))\nexprs = [[D ~ denom]; [A...] .~ soln]\nlatexify(exprs)\n\nbeginalign*\nD = W_0  W_11  W_22 - left( W_12 right)^2  W_0 - left( W_1 right)^2  W_22 + 2  W_1  W_12  W_2 - left( W_2 right)^2  W_11 \nA_0 = fracW_11  W_22 - left( W_12 right)^2D \nA_1 = fracW_1  W_22 - W_12  W_2 - D \nA_2 = fracW_1  W_12 - W_11  W_2D\nendalign*","category":"section"},{"location":"explain/#Explanation","page":"Explanation","title":"Explanation","text":"Pages = [\"explain.md\"]\nDepth = 2:2","category":"section"},{"location":"explain/#Estimator-Pipeline","page":"Explanation","title":"Estimator Pipeline","text":"","category":"section"},{"location":"explain/#Direct-Comparisons","page":"Explanation","title":"Direct Comparisons","text":"The following figures provide direct comparisons of the four major steps in the estimator pipeline described above through their visual impact on a few example distributions.\n\nLinearBinning: The data is histogrammed onto a uniformly spaced grid.\nFor visualization purposes, the histograms are plotted with bins with a width equal to the automatically determined bandwidth (see Bandwidth Estimators below) for the distribution, whereas the remaining panels use 8Ã— as many data points to achieve a smoother curve.\nBasicKDE: This step convolves with the histogram with the Gaussian kernel in order to smooth the data from a discontinuous histogram into a smooth curve.\nThe basic density estimator is sufficient for an unbounded and smooth distribution like the Normal case. In the cases of the HalfNormal and Uniform distributions that have non-zero boundaries, though, the distribution is severely underestimated near the boundaries.\nLinearBoundaryKDE: This step corrects for the boundary effects by recovering both the normalization (due to convolving with implicit zeros beyond the boundary) and recovers the slope of the distribution near boundaries.\nCompared to the BasicKDE step, the HalfNormal and Uniform distributions near their boundaries are significantly improved.\nMultiplicativeBiasKDE: The final stage permits use of a larger bandwidth to achieve a smoother density estimate without sacrificing the sharpness of any curves / peaks. The algorithm automatically increases the bandwidth when the multiplicative bias correction is used.\nThe visual impact is more subtle than in previous stages, but the smoothness of the Uniform and Chisq3 distributions compared to the previous stage are a consequence of the multiplicative bias correction permitting a larger kernel bandwidth (without broadening the peak in the Chisq3 distribution).\n\ndetails: Plotting Code\nusing CairoMakie\nusing Distributions\nusing Random\n\nimport KernelDensityEstimation as KDE\n\ndists = [\n    \"Normal\" => Normal(0.0, 1.0),\n    \"Chisq3\" => Chisq(3.0),\n    \"HalfNormal\" => truncated(Normal(0.0, 1.0); lower = 0.0),\n    \"Uniform\" => Uniform(0.0, 1.0),\n]\n\nestimators = [\n    KDE.LinearBinning(),\n    KDE.BasicKDE(),\n    KDE.LinearBoundaryKDE(),\n    KDE.MultiplicativeBiasKDE(),\n]\n\nfor (name, dist) in dists\n    fig = Figure(size = (900, 400))\n    axs = Axis[]\n\n    for (ii, method) in enumerate(estimators)\n        dohist = method isa KDE.AbstractBinningKDE\n\n        Random.seed!(123)  # hide\n        rv = rand(dist, 5_000)\n        dens = KDE.kde(rv; method, bounds = dist, bwratio = dohist ? 1 : 8)\n\n        ax = Axis(fig[1, ii]; title = string(nameof(typeof(method))),\n                              xlabel = \"value\", ylabel = \"density\")\n        lines!(ax, dist, color = (:black, 0.5), linestyle = :dash, label = \"true\")\n        plotter! = method isa KDE.AbstractBinningKDE ? stairs! : lines!\n        plotter!(ax, dens; label = \"estimate\",\n                           color = dohist ? :blue3 : :firebrick3)\n        scatter!(ax, [0], [0], color = (:black, 0.0))  # transparent dot to stop suppressed y=0 axis\n        ii > 1 && hideydecorations!(ax, grid = false, ticks = false)\n\n        push!(axs, ax)\n    end\n    linkaxes!(axs...)\n\n    Label(fig[0, :], name, font = :bold, fontsize = 20)\n    save(\"comparison_$name.svg\", fig)\nend\nnothing  # hide\n\n(Image: ) (Image: ) (Image: ) (Image: )","category":"section"},{"location":"explain/#Bandwidth-Estimators","page":"Explanation","title":"Bandwidth Estimators","text":"","category":"section"},{"location":"releasenotes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"Pages = [\"releasenotes.md\"]\nDepth = 2:2\n\n","category":"section"},{"location":"releasenotes/#v0.8.0-â€”-Unreleased","page":"Release Notes","title":"v0.8.0 â€” Unreleased","text":"The package extension for UnicodePlots.jl no longer defines the 3-argument Base.show method for the UnivariateKDE type in favor of requiring the user to explicitly plot it in the terminal with UnicodePlots.lineplot\nThe package extension for RecipesBase.jl has been added to aid in plotting within the Plots.jl ecosystem.\nVarious changes to the interfaces and type definitions have been made to allow for future support of bivariate (and possibly multivariate) density estimates.\nThe boundary function has been removed, with all of its functionality subsumed by the bounds function, which has also impacted the built-in definitions and behaviors of the bounds methods.\nThe signatures of the bandwidth and estimate interfaces have been changed to support multidimensional data. More notably, the returned bandwidth is now always a unitless quantity (since a covariance matrix among axes with different units cannot have per-element units).\nThe type parameterizations of AbstractKDE and UnivariateKDE have changed in a backwards-incompatible way.\nPreviously, the univariate structure had four type parameters and subtype relationship,\nUnivariateKDE{T_input, T_density,\n              R<:AbstractRange{T_input},\n              V<:AbstractVector{T_density}\n             } <: AbstractKDE{T_input}\nwhere the first two parameters are the element types of the input data and output density estimate, respectively. The trailing two then specialize the container types of the binning and density vectors, but critically they use separate type variables in order to support unitful quantities where the units are inverses of one another. Notably, the type relationship to the abstract supertype was declared to use the input element type.\nThe new type definition is instead an alias of the more generic MultivariateKDE struct and takes the form\nUnivariateKDE{T_density,\n              R<:AbstractRange,\n              V<:AbstractVector{T_density}\n             } <: AbstractKDE{T_density}\nThe trailing two type parameters are the same, but now the output element type of the density array is used in the supertype relationship. The input element type also no longer appear as an explicit type parameter.\nThese changes are required to align with a future definition of multivariate density estimates. The explicit input element type is dropped since each axis may have different element type (i.e. units) and are implicitly available via the axis range(s). In contrast, the element type of the density array is unique, so this is the natural choice to use in the supertype relationship.\nUnivariateKDEInfo has been redefined as an alias of the more generic MultivariateKDEInfo struct.\nAn experimental MultivariateKDE type has been added to support higher dimensional density estimation. UnivariateKDE and BivariateKDE are type aliases for the 1- and 2-dimensional cases.\n\n","category":"section"},{"location":"releasenotes/#v0.7.0-â€”-2025-Jun-08","page":"Release Notes","title":"v0.7.0 â€” 2025 Jun 08","text":"A new weights keyword argument has been added to the kde function to support weighted data sets. As a consequence, the API of multiple interfaces have been changed:\nbandwidth has gained a weights keyword.\nThe init method returns three values (data, weights, and info) instead of just two.\nThe estimate function takes a mandatory weights positional argument.\nRevert the \"accurate histogram\" binning calculation made in the previous release. Further testing has shown that much of the extra work being done was ineffective, so no promise is currently made about being able to precisely bin range(lo, hi, nbins) (nor LinRange(lo, hi, len)).\n\n","category":"section"},{"location":"releasenotes/#v0.6.0-â€”-2024-Dec-31","page":"Release Notes","title":"v0.6.0 â€” 2024 Dec 31","text":"Public functions have been declared using Julia v1.11+'s public keyword.\nThe value of the bin-center for the zero-width singleton histogram has been fixed.\nImplement more accurate histogram (and linear) binning calculations. For the HistogramBinning case, it is now possible to precisely bin range(lo, hi, nbins) values into their corresponding bins (whereas previously values may be counted incorrectly one bin too low due to rounding in the floating point calculations). (Reverted in v0.7.0)\nThe implementation has been modified to support unitful quantities (without adding a new package dependency) via careful consideration and application of appropriate factors of one and/or oneunit (and relaxing type constraints or adding new type parameters to structs, where necessary). Given a vector with units u, the density object's fields (K.x, K.f) have units (u, u^-1), respectively, in correspondence with interpreting any integrated range to be a unitless probability (i.e. probability = sum(K.f[a .< K.x .< b]) * step(K.x)).\nThe package no longer depends on Roots.jl (used by the ISJ bandwidth estimator); instead, an implementation of Brent's Method is now included here directly. This not only decreases the dependence on external packages but also reduces both package load time and the precompiled package image size.\nThe documentation has generally been improved:\nA new \"Showcase\" section has been added to showcase examples of density estimation with this package.\nA User Guide has been started to give a brief introduction to installing and using the package.\nThe documentation now includes release notes.\n\n","category":"section"},{"location":"releasenotes/#v0.5.0-â€”-2024-Nov-21","page":"Release Notes","title":"v0.5.0 â€” 2024 Nov 21","text":"Have the ISJ bandwidth estimator fallback to Silverman rule automatically when it fails to converge. This is expected to happen for very flat (closed) distributions, so it's not as rare of an occurrence as originally understood.\nAdd package extension to aid in plotting with Makie.jl.\nBegin adding more extensive documentation to the package:\nAdd a README to give brief justification for the package.\nDescribe the package extensions available and what features they provide.\nDemonstrate the impact of the different stages of the estimator pipeline by comparing each stage for several example distributions.\n\n","category":"section"},{"location":"releasenotes/#v0.4.0-â€”-2024-Sep-09","page":"Release Notes","title":"v0.4.0 â€” 2024 Sep 09","text":"Add an interface method boundary which can be overloaded to implement mechanisms for automatically determining appropriate boundary conditions.\nThe two built-in methods are to convert from symbols to enum (e.g. :open to Open) and to infer the boundary conditions from a 2-tuple of finite/infinite real values.\nAdd an interface method bounds (and eponymous keyword argument to kde) which can be overloaded to implement mechanisms for automatically both the boundary condition (keyword boundary) and limits (keywords lo and hi) from an arbitrary value.\nStore more information within the UnivariateKDEInfo structure. The init uses the new fields to pass relevant parameters to later stages of the estimator pipeline.\nOn Julia v1.9+, new extension packages have been added to integrate with external packages:\nThe aforementioned boundary and bounds methods have been specialized for univariate distributions from Distributions.jl\nThe UnicodePlots.jl package, if loaded, is used to visualize the kernel density estimate at the terminal.\nIncrease the automatic bandwidth determined by the chosen bandwidth estimator for higher-order estimators (such as MultiplicativeBiasKDE) which have a lower level of bias. (See Lewis [2], Â§E, Eqn 35 and Footnote 10 for further details.)\nRename the boundary condition enum from Cover to Boundary.\nFix syntax or usage errors that broke compatibility with Julia v1.6.\n\nA. Lewis. GetDist: a Python package for analysing Monte Carlo samples (2019), arXiv:1910.13970.\n\n","category":"section"},{"location":"releasenotes/#v0.3.0-â€”-2024-Jul-21","page":"Release Notes","title":"v0.3.0 â€” 2024 Jul 21","text":"This release adds the ISJ bandwidth estimator described in Botev et al. [4] and uses it by default, as it is more capable of dealing both with non-Gaussian distributions and respects the complexity/nature of bounded domains.\n\nAdd an implementation of the Improved Sheather-Jones bandwidth estimator.\nRename the interface method for bandwidth estimators to bandwidth.\nConsolidate data and option pre-processing into the init method, which is the first step in the density estimation pipeline.\n\nZ. Botev, J. Grotowski and D. Kroese. Kernel density estimation via diffusion.  The Annals of Statistics 38 (2010),  arXiv:1011.2602.\n\n","category":"section"},{"location":"releasenotes/#v0.2.0-â€”-2024-Jul-19","page":"Release Notes","title":"v0.2.0 â€” 2024 Jul 19","text":"Require Julia v1.6+.\nImproved docstrings throughout.\nAdded framework for building documentation with Documenter.jl.\nMigrate specific density estimation implementations to be methods of the (new) estimate interface function (rather than overloading kde).\nFix handing of edge-case where a constant vector is given. For closed boundary conditions, the result is a zero-width singleton bin.\nFix error in widening of the KDE range based on the kernel bandwidth.\n\n","category":"section"},{"location":"releasenotes/#v0.1.0-â€”-2024-Jul-13","page":"Release Notes","title":"v0.1.0 â€” 2024 Jul 13","text":"Initial release supports:\n\nUnivariate kernel density estimation.\n2 binning methods (histogramming and linear binning) and 3 density estimation techniques (basic, with linear boundary correction, and/or with multiplicative bias correction\nSupport for distributions with (half-)closed boundary conditions.\nAutomatic bandwidth selection using Silverman's rule.","category":"section"},{"location":"showcase/bk18_likelihood/#CosmoMC-Weighted-Chains-(BK18-baseline-likelihood-analysis)","page":"CosmoMC Weighted Chains (BK18 baseline likelihood analysis)","title":"CosmoMC Weighted Chains (BK18 baseline likelihood analysis)","text":"The figure below is to be compared to Figure 4 from BICEP/Keck paper XIII.\n\nAfter loading the appropriate columns from the set of MCMC chains and thinning, performing the kernel density estimation is as simple as:\n\nK_r  = kde(chain_r;  weights = chain_weight, lo =  0.0,            boundary = :closedleft)\nK_Ad = kde(chain_Ad; weights = chain_weight, lo =  0.0,            boundary = :closedleft)\nK_As = kde(chain_As; weights = chain_weight, lo =  0.0,            boundary = :closedleft)\nK_Î²d = kde(chain_Î²d; weights = chain_weight, lo =  0.8, hi =  2.4)\nK_Î²s = kde(chain_Î²s; weights = chain_weight, lo = -4.5, hi = -2.0)\nK_Î±d = kde(chain_Î±d; weights = chain_weight, lo = -1.0, hi =  0.0, boundary = :closed)\nK_Î±s = kde(chain_Î±s; weights = chain_weight, lo = -1.0, hi =  0.0, boundary = :closed)\nK_Îµ  = kde(chain_Îµ;  weights = chain_weight, lo = -1.0, hi =  1.0, boundary = :closed)\n\n(Image: )\n\nNote that because this package does not support constructing 2D density estimates, the 68% and 95% confidence \"ellipses\" in the lower-left triangle of the plot has been replaced with simple 2D histograms. The hex bin sizes have been chosen using 2Ã— the automatically-determined bandwidths of the corresponding 1D curves.\n\nMain.@showcase_source","category":"section"},{"location":"api/#API","page":"API","title":"API","text":"Pages = [\"api.md\"]\nDepth = 2:2","category":"section"},{"location":"api/#User-Interface","page":"API","title":"User Interface","text":"","category":"section"},{"location":"api/#Advanced-User-Interface","page":"API","title":"Advanced User Interface","text":"","category":"section"},{"location":"api/#Binning-Methods","page":"API","title":"Binning Methods","text":"","category":"section"},{"location":"api/#Density-Estimation-Methods","page":"API","title":"Density Estimation Methods","text":"","category":"section"},{"location":"api/#Bandwidth-Estimators","page":"API","title":"Bandwidth Estimators","text":"","category":"section"},{"location":"api/#Interfaces","page":"API","title":"Interfaces","text":"","category":"section"},{"location":"api/#Density-Estimation-Methods-2","page":"API","title":"Density Estimation Methods","text":"","category":"section"},{"location":"api/#KernelDensityEstimation.kde","page":"API","title":"KernelDensityEstimation.kde","text":"estim = kde(data...;\n            weights = nothing, method = MultiplicativeBiasKDE(),\n            lo = nothing, hi = nothing, boundary = :open, bounds = nothing,\n            bandwidth = ISJBandwidth(), bwratio = 8 nbins = nothing)\n\nCalculate a discrete kernel density estimate (KDE) estim from samples data, optionally weighted by a corresponding vector of weights.\n\nThe default method of density estimation uses the MultiplicativeBiasKDE pipeline, which includes corrections for boundary effects and peak broadening which should be an acceptable default in many cases, but a different AbstractKDEMethod can be chosen if necessary.\n\nThe interval of the density estimate can be controlled by either the set of lo, hi, and boundary keywords or the bounds keyword, where the former are conveniences for setting bounds = (lo, hi, boundary). The minimum and maximum of v are used if lo and/or hi are nothing, respectively. (See also bounds.)\n\nThe KDE is constructed by first histogramming the input v into nbins many bins with outermost bin edges spanning lo to hi. The span of the histogram may be expanded outward based on boundary condition, dictating whether the boundaries are open or closed. The bwratio parameter is used to calculate nbins when it is not given and corresponds (approximately) to the ratio of the bandwidth to the width of each histogram bin.\n\nAcceptable values of boundary are:\n\n:open or Open\n:closed or Closed\n:closedleft, :openright, ClosedLeft, or OpenRight\n:closedright, :openleft, ClosedRight, or OpenLeft\n\nThe histogram is then convolved with a Gaussian distribution with standard deviation bandwidth. The default bandwidth estimator is the Improved Sheather-Jones (ISJBandwidth) if no explicit bandwidth is given.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.MultivariateKDE","page":"API","title":"KernelDensityEstimation.MultivariateKDE","text":"MultivariateKDE{T, N, R<:Tuple{Vararg{AbstractRange,N}, V<:AbstractVector{T}} <: AbstractKDE{T, N}\n\nFields\n\naxes::R: An N-tuple of the locations (bin centers) along the axes of the density estimate. Each axis is a range type with uniform step size.\ndensity::V: An N-dimensional array with element type T of the density estimate values.\n\nSee also UnivariateKDE and BivariateKDE.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.UnivariateKDE","page":"API","title":"KernelDensityEstimation.UnivariateKDE","text":"UnivariateKDE{T, R, V} = MultivariateKDE{T, 1, Tuple{R}, V}\n\nA simplifying alias of a 1-dimensional MultivariateKDE structure.\n\nProperties\n\nThe following properties are defined to supplement the fields of the underlying MultivariateKDE struct.\n\nx: An alias for the first (and only) axis; i.e. K.x == K.axes[1]\nf: An alias for the name density (for backwards compatibility).\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.BivariateKDE","page":"API","title":"KernelDensityEstimation.BivariateKDE","text":"BivariateKDE{T, R, V} = MultivariateKDE{T, 2, R, V}\n\nA simplifying alias of a 2-dimensional MultivariateKDE structure.\n\nProperties\n\nThe following properties are defined to supplement the fields of the underlying MultivariateKDE struct.\n\nx: An alias for the first axis; i.e. K.x == K.axes[1]\ny: An alias for the first (and last) axis; i.e. K.y == K.axes[2]\nf: An alias for the name density (for consistency with UnivariateKDE)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.Boundary","page":"API","title":"KernelDensityEstimation.Boundary","text":"@enum T Closed Open ClosedLeft ClosedRight\nconst OpenLeft = ClosedRight\nconst OpenRight = ClosedLeft\n\nEnumeration to describe the desired boundary conditions of the domain of the kernel density estimate K. For some given data d  a b, the boundary conditions have the following impact:\n\nClosed: The domain K  a b is used directly as the bounds of the binning.\nOpen: The desired domain K  (- +) is effectively achieved by widening the bounds of the data by the size of the finite convolution kernel. Specifically, the binning is defined over the range a - 8Ïƒ b + 8Ïƒ where Ïƒ is the bandwidth of the Gaussian convolution kernel.\nClosedLeft: The left half-closed interval K  a +) is used as the bounds for binning by adjusting the upper limit to the range a b + 8Ïƒ. The equivalent alias OpenRight may also be used.\nClosedRight: The right half-closed interval K  (- b is used as the bounds for binning by adjusting the lower limit to the range a - 8Ïƒ b. The equivalent alias OpenLeft may also be used.\n\n\n\n\n\n","category":"module"},{"location":"api/#KernelDensityEstimation.init","page":"API","title":"KernelDensityEstimation.init","text":"data, weights, details = init(method::K,\n        data::Tuple{Vararg{AbstractVector,N}},\n        weights::Union{Nothing,<:AbstractVector} = nothing;\n        bounds = nothing,\n        bwratio::Union{Nothing,Tuple{Vararg{Real,N}}} = nothing,\n        nbins::Union{Nothing,Tuple{Vararg{Integer,N}}} = nothing,\n        bandwidth::Union{<:Number,<:AbstractBandwidthEstimator} = ISJBandwidth(),\n        kwargs...\n    ) where {K<:AbstractKDEMethod, N}\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.AbstractBinningKDE","page":"API","title":"KernelDensityEstimation.AbstractBinningKDE","text":"AbstractBinningKDE <: AbstractKDEMethod\n\nThe abstract supertype of data binning methods which are the first step in the density estimation process. The two supported binning methods are HistogramBinning and LinearBinning.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.HistogramBinning","page":"API","title":"KernelDensityEstimation.HistogramBinning","text":"struct HistogramBinning <: AbstractBinningKDE end\n\nBase case which generates a density estimate by histogramming the data.\n\nSee also LinearBinning\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.LinearBinning","page":"API","title":"KernelDensityEstimation.LinearBinning","text":"struct LinearBinning <: AbstractBinningKDE end\n\nBase case which generates a density estimate by linear binning of the data.\n\nSee also HistogramBinning\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.BasicKDE","page":"API","title":"KernelDensityEstimation.BasicKDE","text":"BasicKDE{M<:AbstractBinningKDE} <: AbstractKDEMethod\n\nA baseline density estimation technique which convolves a binned dataset with a Gaussian kernel truncated at its 4Ïƒ bounds.\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.LinearBoundaryKDE","page":"API","title":"KernelDensityEstimation.LinearBoundaryKDE","text":"LinearBoundaryKDE{M<:AbstractBinningKDE} <: AbstractKDEMethod\n\nA method of KDE which applies the linear boundary correction of Jones and Foster [1] as described in Lewis [2] after BasicKDE density estimation. This correction primarily impacts the KDE near a closed boundary (see Boundary) and has the effect of improving any non-zero gradient at the boundary (when compared to normalization corrections which tend to leave the boundary too flat).\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.MultiplicativeBiasKDE","page":"API","title":"KernelDensityEstimation.MultiplicativeBiasKDE","text":"MulitplicativeBiasKDE{B<:AbstractBinningKDE,M<:AbstractKDEMethod} <: AbstractKDEMethod\n\nA method of KDE which applies the multiplicative bias correction described in Lewis [2]. This correction is designed to reduce the broadening of peaks inherent to kernel convolution by using a pilot KDE to flatten the distribution and run a second iteration of density estimation (since a perfectly uniform distribution cannot be broadened further).\n\nFields and Constructor Keywords\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of density estimation. Defaults to HistogramBinning().\nmethod::AbstractKDEMethod: The KDE method to use for the pilot and iterative density estimation. Defaults to LinearBoundaryKDE().\n\nNote that if the given method has a configurable binning type, it is ignored in favor of the explicit binning chosen.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.AbstractBandwidthEstimator","page":"API","title":"KernelDensityEstimation.AbstractBandwidthEstimator","text":"AbstractBandwidthEstimator\n\nAbstract supertype of kernel bandwidth estimation techniques.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.SilvermanBandwidth","page":"API","title":"KernelDensityEstimation.SilvermanBandwidth","text":"SilvermanBandwidth <: AbstractBandwidthEstimator\n\nEstimates the necessary bandwidth of data at d-dimensional coordinates (ð’—_1 ð’—_2  ð’—_d) with weights ð’˜ using Silverman's Rule for a Gaussian smoothing kernel.\n\nFor the univariate (d = 1) case:\n\n    h = left(frac43n_mathrmeffright)^15 ÏƒÌ‚\n\nwhere n_mathrmeff is the effective number of degrees of freedom of the data and ÏƒÌ‚^2 is its sample variance.\n\nIn the multivariate case (d  2):\n\n    ð’‰ = left(frac4(2 + d)n_mathrmeffright)^1(4 + d) sqrtðœ®Ì‚\n\nwhere sqrtðœ®Ì‚ is a Cholesky decomposition of the weighted sample covariance.\n\nSee also ISJBandwidth\n\nExtended help\n\nThe sample (co)variance and effective number of degrees of freedom are calculated using weighted statistics, where the latter is defined to be Kish's effective sample size n_mathrmeff = (sum_i ð’˜_i)^2  sum_i ð’˜_i^2. For uniform weights, this reduces to the length of the vector(s) ð’—_j.\n\nReferences\n\nHansen [3]\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.ISJBandwidth","page":"API","title":"KernelDensityEstimation.ISJBandwidth","text":"ISJBandwidth <: AbstractBandwidthEstimator\n\nEstimates the necessary bandwidth of a vector of data v using the Improved Sheather-Jones (ISJ) plug-in estimator of Botev et al. [4].\n\nThis estimator is more capable of choosing an appropriate bandwidth for bimodal (and other highly non-Gaussian) distributions, but comes at the expense of greater computation time and no guarantee that the estimator converges when given very few data points.\n\nSee also SilvermanBandwidth\n\nFields\n\nbinning::AbstractBinningKDE: The binning type to apply to a data vector as the first step of bandwidth estimation. Defaults to HistogramBinning().\nbwratio::Int: The relative resolution of the binned data used by the ISJ plug-in estimator â€” there are bwratio bins per interval of size hâ‚€, where the intial rough initial bandwidth estimate is given by the SilvermanBandwidth estimator. Defaults to 2.\nniter::Int: The number of iterations to perform in the plug-in estimator. Defaults to 7, in accordance with Botev et. al. who state that higher orders show little benefit.\nfallback::Bool: Whether to fallback to the SilvermanBandwidth if the ISJ estimator fails to converge. If false, an exception is thrown instead.\n\nReferences\n\nBotev et al. [4]\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.bandwidth","page":"API","title":"KernelDensityEstimation.bandwidth","text":"h = bandwidth(estimator::AbstractBandwidthEstimator,\n              data::Tuple{Vararg{AbstractVector,N}},\n              bounds::Tuple{Vararg{Tuple{Any,Any,Boundary.T},N}},\n              weights::Union{Nothing,<:AbstractVector} = nothing\n              ) where {N}\n\nDetermine the appropriate bandwidth h of the data set data (optionally with corresponding weights) using chosen estimator algorithm. The bandwidth is provided the domain and boundary conditions (see bounds) of the requested KDE for use in filtering and/or correctly interpreting the data, if necessary.\n\nExtended help\n\nThe bandwidth is the standard deviation of the Gaussian smoothing kernel.\n\nFor univariate (N = 1) densities, h must be a unitless scalar.\nFor multivariate (N  2) densities, h must be a (unitless) Cholesky factorization of the corresponding multivariate covariance matrix.\nhint: Hint\nThe lower-triangular Cholesky factor is used during kernel construction, so opting for the lower-triangular symmetric form is suggested for multivariate estimator implementations:h = cholesky(Symmetric(covariance, :L))\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.AbstractKDE","page":"API","title":"KernelDensityEstimation.AbstractKDE","text":"AbstractKDE{T,N}\n\nAbstract supertype of kernel density estimates with element type T and dimensionality N.\n\nSee also UnivariateKDE\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.AbstractKDEInfo","page":"API","title":"KernelDensityEstimation.AbstractKDEInfo","text":"AbstractKDEInfo{T,N}\n\nAbstract supertype of auxiliary information used during kernel density estimation.\n\nSee also UnivariateKDEInfo\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.MultivariateKDEInfo","page":"API","title":"KernelDensityEstimation.MultivariateKDEInfo","text":"MultivariateKDEInfo{U,N} <: AbstractKDEInfo{U,N}\n\nInformation about the density estimation process, providing insight into both the entrypoint parameters and some internal state variables.\n\nExtended help\n\nType parameters\n\nU: A unitless element type compatible with the density estimate.\nN: The dimensionality of the density estimate.\n\nFields\n\nmethod::AbstractKDEMethod: The estimation method used to generate the KDE.\nbounds::Any: The bounds specification of the estimate as passed to init(), prior to making it concrete via calling bounds().\ndomain::Union{Nothing, Tuple{Vararg{Tuple{Ei,Ei,Boundary.T} where Ei,N}}}: A tuple of the concrete range and boundary conditions of the density estimate axes after calling bounds() with the value of the .bounds field before adding any requisite padding for open boundary conditions.\nbwratio::Union{Nothing, NTuple{N,U}}: The ratio between the bandwidth and the width of a histogram bin for each axis, used only when the number of bins is not explicitly provided.\nnbins::Union{Nothing,NTuple{N,Int}}: The number of requested bins along each axis. If nothing, then the number of bins is calculated using the padded domain of the density estimate, the bandwidth, and the ratio .bwratio.\nneffective::U: Kish's effective sample size of the data, which equals the number of samples for uniformly weighted data.\nbandwidth_alg::Union{Nothing,AbstractBandwidthEstimator}: Algorithm used to estimate an appropriate bandwidth, if a concrete value was not provided to the estimator, otherwise nothing.\nbandwidth::Union{Nothing,U,<:AbstractMatrix{U}}: The bandwidth of the convolution kernel.\nkernel::Union{Nothing,MultivariateKDE{U,N}}: The convolution kernel used to smooth the density estimate.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.UnivariateKDEInfo","page":"API","title":"KernelDensityEstimation.UnivariateKDEInfo","text":"UnivariateKDEInfo{U, D, B, K} = MultivariateKDEInfo{U, 1, D, B, K}\n\nA simplifying alias of a 1-dimensional MultivariateKDEInfo structure.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.BivariateKDEInfo","page":"API","title":"KernelDensityEstimation.BivariateKDEInfo","text":"BivariateKDEInfo{U, D, B, K} = MultivariateKDEInfo{U, 1, D, B, K}\n\nA simplifying alias of a 2-dimensional MultivariateKDEInfo structure.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.AbstractKDEMethod","page":"API","title":"KernelDensityEstimation.AbstractKDEMethod","text":"AbstractKDEMethod\n\nThe abstract supertype of all kernel density estimation methods, including the data binning process (see AbstractBinningKDE) and subsequent density estimation techniques (such as BasicKDE).\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelDensityEstimation.bounds","page":"API","title":"KernelDensityEstimation.bounds","text":"domain = bounds(xs::NTuple{N,AbstractVector}, spec) where {N}\n\nDetermine the appropriate domain (lower and upper bounds and boundary condition in each dimension) given the data vectors xs and bounds specification spec. The returned value must be an N-tuple, containing tuple elements of the form (lo::Real, hi::Real, bc::Boundary.T).\n\nPackages may specialize this method on the spec argument to modify the behavior of the interval and boundary refinement for new argument types.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.estimate","page":"API","title":"KernelDensityEstimation.estimate","text":"estim, info = estimate(method::AbstractKDEMethod,\n                       data::Tuple{Vararg{AbstractVector,N}},\n                       weights::Union{Nothing, AbstractVector}; kwargs...) where {N}\n\nestim, info = estimate(method::AbstractKDEMethod,\n                       data::AbstractKDE,\n                       info::AbstractKDEInfo; kwargs...)\n\nApply the kernel density estimation algorithm method to the given data, either in the form of a tuple of vectors of data (and optionally with corresponding vector of weights) or a previously-processed density estimate and its corresponding info (to support being part of a processing pipeline).\n\nReturns\n\nestim::AbstractKDE: The resultant kernel density estimate.\ninfo::AbstractKDEInfo: Auxiliary information describing details of the density estimation either useful or necessary for constructing a pipeline of processing steps.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelDensityEstimation.estimator_order","page":"API","title":"KernelDensityEstimation.estimator_order","text":"p = estimator_order(::Type{<:AbstractKDEMethod})\n\nThe bias scaning of the density estimator method, where a return value of p corresponds to bandwidth-dependent biases of the order mathcalO(h^2p).\n\n\n\n\n\n","category":"function"},{"location":"showcase/#Showcase","page":"Showcase","title":"Showcase","text":"","category":"section"},{"location":"showcase/#[Simple-Distributions](showcase/simple_distributions/index.md)","page":"Showcase","title":"Simple Distributions","text":"(Image: )","category":"section"},{"location":"showcase/#[CosmoMC-Weighted-Chains-(BK18-baseline-likelihood-analysis)](showcase/bk18_likelihood/index.md)","page":"Showcase","title":"CosmoMC Weighted Chains (BK18 baseline likelihood analysis)","text":"(Image: )","category":"section"},{"location":"showcase/simple_distributions/#showcase_simple","page":"Simple distributions","title":"Simple distributions","text":"(Image: )\n\nMain.@showcase_source","category":"section"},{"location":"userguide/#User-Guide","page":"User Guide","title":"User Guide","text":"Pages = [\"userguide.md\"]\nDepth = 2:2","category":"section"},{"location":"userguide/#Getting-Started","page":"User Guide","title":"Getting Started","text":"To install KernelDensityEstimation.jl, it is recommended that you use the jmert/Registry.jl package registry, which will let you install (and depend on) the package similarly to any other Julia package in the default General registry.\n\npkg> registry add https://github.com/jmert/Registry.jl\n\npkg> add KernelDensityEstimation","category":"section"},{"location":"userguide/#Simple-kernel-density-estimate","page":"User Guide","title":"Simple kernel density estimate","text":"For the following example, we'll use a small sample of Gaussian deviates:\n\nusing KernelDensityEstimation\nx = 3 .+ 0.1 .* randn(250) # x ~ Normal(3, 0.1)\nnothing  # hide\n\nThe key interface of this package is the kde function. In its simplest incantation, you provide a vector of data and it returns a kernel density object (in the form of a UnivariateKDE structure).\n\nusing KernelDensityEstimation\n\nK = kde(x)\nnothing  # hide\n\nThe density estimate f(x) is given at locations K.x (as a StepRangeLen) with density values K.f. For instance, the mean and variance of the distribution are:\n\nÎ¼1 = step(K.x) * sum(@. K.f * K.x)\nÎ¼2 = step(K.x) * sum(@. K.f * K.x^2)\n\n(; mean = Î¼1, std = sqrt(Î¼2 - Î¼1^2))\n\nwhich agree well with the known underlying parameters (mu = 3 sigma = 01).\n\nVisualizing the density estimate (see Extensions â€” Makie.jl), we see a fair level of consistency between the density estimate and the known underlying model.\n\n(Image: )\n\n\n","category":"section"},{"location":"userguide/#Densities-with-boundaries","page":"User Guide","title":"Densities with boundaries","text":"The previous example arises often and is handled well by most kernel density estimation solutions. Being a Gaussian distribution makes it particularly well behaved, but in general distributions which are unbounded and gently fade away to zero towards pminfty are relatively easy to deal with. Despite how often the Gaussian distribution is an appropriate [approximation of the] distribution, there are still many cases where various bounded distributions are expected, and ignoring the boundary conditions can lead to a very poor density estimate.\n\nTake the simple case of the uniform distribution on the interval 0 1.\n\nRandom.seed!(101)  # hide\nx = rand(5_000)\nnothing  # hide\n\nBy default, kde assumes the distribution is unbounded, and this leads to \"smearing\" the density across the known boundaries to the regions x  0 and x  1:\n\nK0 = kde(x)\nnothing  # hide\n\n(Image: )\n\n\n\n\nWe can inform the estimator that we expect a bounded distribution, and it will use that information to generate a more appropriate estimate. To do so, we make use of three keyword arguments in combination:\n\nlo to dictate the lower bound of the data.\nhi to dictate the upper bound of the data.\nboundary to specify the boundary condition, such as :open (unbounded), :closed (finite), and half-open intervals :closedleft/:openright and :closedright/:openright.\n\nIn this example, we know our data is bounded on the closed interval 0 1, so we can improve the density estimate by providing that information\n\nK1 = kde(x, lo = 0, hi = 1, boundary = :closed)\nnothing  # hide\n\n(Image: )\n\n\n\n\nNote that in addition to preventing the smearing of the density beyond the bounds of the known distribution, the density estimate with correct boundaries is also smoother than the unbounded estimate. This is because the sharp drops at x = 0 1 no longer need to be represented, so the algorithm is no longer compromising on smoothing the interior of the distribution with retaining the cut-offs.\n\nhint: Hint\nIn addition to the aforementioned triple of lo, hi, and boundary keywords, there is a single bounds keyword which can replace all three. The built-in mechanism only accepts a tuple where bounds = (lo, hi, boundary), but the additional keyword makes it possible to customize behavior for new types of arguments. For example, there is a package extension for Distributions.jl which allows using the support of a distribution to automatically infer appropriate boundary conditions and limits.See the docstring for kde (and references therein) for more information.","category":"section"},{"location":"userguide/#Densities-of-weighted-samples","page":"User Guide","title":"Densities of weighted samples","text":"In some cases, the data to be analyzed is a weighted vector of data (represented as a vector of data and a corresponding vector of weight factors). For instance, importance sampling of an MCMC chain results in non-uniform weights that then must be considered when deriving a density estimate.\n\nTake the following toy example where we have a target parameter v and nuisance parameter p that are correlated, where a uniform prior was assumed for p:\n\nRandom.seed!(200)  # hide\n# correlation coefficient and nuisance parameter\nÏ, p = 0.85, randn(500)\n# correlated target parameter\nv = Ï .* p .+ sqrt(1 - Ï^2) .* randn.()\nnothing  # hide\n\nNow suppose that you have reason to update your prior on p, believing now that positive values are twice as likely as negative ones. If the method of generating v is expensive, and because the change in prior is not extreme, it may be efficient and acceptable to instead importance sample the existing values by reweighting the samples by the ratio of the priors:\n\nbeginalign*\nP_1(p) propto 1\n\nP_2(p) propto begincases\n    1  p  0 \n    2  p ge 0 \n    endcases\nendalign*\n\nP1(z) = 1.0\nP2(z) = z â‰¥ 0 ? 2.0 : 1.0\nweights = P2.(p) ./ P1.(p)\nnothing  # hide\n\nWe then simply provide these weights as a keyword argument in the call to kde:\n\nK1 = kde(v)\nK2 = kde(v; weights)\nnothing  # hide\n\n(Image: )\n\n\n\n\nAs expected, this shifts the resultant density estimate to the right, toward more positive values.\n\nnote: Note\nThe effective sample size (UnivariateKDEInfo.neffective) is calculated from the weights using Kish's definition. Both of the bandwidth estimators (SilvermanBandwidth and ISJBandwidth) use this definition in scaling the bandwidth with the (effective) sample size.","category":"section"},{"location":"#Kernel-Density-Estimation","page":"Kernel Density Estimation","title":"Kernel Density Estimation","text":"import Markdown\nreadmetxt = read(joinpath(dirname(@__FILE__), \"..\", \"..\", \"README.md\"), String)\nreadme = Markdown.parse(readmetxt)\n\n# Keep the contents between the title heading and the first horizontal rule (exclusive)\nii = findfirst(x -> x isa Markdown.Header{1}, readme.content)\njj = findfirst(x -> x isa Markdown.HorizontalRule, readme.content)\nreadme.content = readme.content[ii+1:jj-1]\n\nreadme","category":"section"},{"location":"#Why-another-kernel-density-estimation-package?","page":"Kernel Density Estimation","title":"Why another kernel density estimation package?","text":"As of Nov 2024, much of the Julia ecosystem uses the KernelDensity.jl package (possibly implicitly, such as through density plots in Makie.jl, StatsPlots.jl, etc).\n\nConsider the following (toy) examples: one case where we have samples drawn from a Gaussian distribution, and a second where only the positive values are retained.\n\nusing Random\nRandom.seed!(1234)  # hide\n\n# A vector of Gaussian random deviates\nrv_gauss = randn(500)\n# and its expected distribution\nx = -5.0:0.01:5.0\nexp_gauss = @. exp(-x^2 / 2) / sqrt(2Ï€)\n\n# Then filter the random deviates to be strictly positive\nrv_trunc = filter(>(0.0), rv_gauss)\n# and its corresponding distribution (Ã—2 to keep normalization)\nexp_trunc = @. ifelse(x > 0.0, 2exp_gauss, 0.0)\nnothing  # hide\n\ndetails: Plotting Setup\nusing CairoMakie\n\nfunction draw_KD(grid, rv, (x, exp_dist), (z, kde_dist), title)\n    ax = Axis(grid, title = title)\n    # draw the reference expectation distribution\n    lines!(ax, x, exp_dist, linestyle = :dash, color = :black)\n    # draw the kernel density estimate\n    lines!(ax, z, kde_dist, linewidth = 2, color = Cycled(1))\n\n    # add a shadow axis and marks along the bottom edge to indicate\n    # the location of the random deviates\n    ax2 = Axis(grid, limits = (nothing, (0.0, 1.0)))\n    vlines!(ax2, rv, ymin=0.0, ymax=0.03, linewidth = 0.5, color = (:black, 0.2))\n    hidedecorations!(ax2)\n    hidespines!(ax2)\n    linkxaxes!(ax2, ax)\n\n    # fix the range of the axes\n    xlims!(ax2, -5.9, 5.9)\nend\nnothing  # hide\n\nIf we then plot the outputs of running the KernelDensity.kde method on each of these two vectors:\n\nimport KernelDensity as KD\n\nkd_gauss = KD.kde(rv_gauss)\nkd_trunc = KD.kde(rv_trunc)\nnothing  # hide\n\n(Image: )\nKernel density estimates for a full (left) and truncated (right) Gaussian samples as produced using the default settings from KernelDensity.jl.\n\n\n\ndetails: Plotting Code\nfig = Figure(size = (800, 400))\n\n# Gaussian distribution & KDE\nref = (x, exp_gauss)\nkd = (kd_gauss.x, kd_gauss.density)\ndraw_KD(fig[1, 1], rv_gauss, ref, kd, \"Gaussian\")\n\n# Truncation Gaussian distribution & KDE\nref = (x, exp_trunc)\nkd = (kd_trunc.x, kd_trunc.density)\ndraw_KD(fig[1, 2], rv_trunc, ref, kd, \"Truncated Gaussian\")\n\nLabel(fig[0, :], \"KernelDensity.jl\", font = :bold, fontsize = 20)\n\nsave(\"example_kerneldensity.svg\", fig)  # hide\nnothing  # hide\n\nFor the Gaussian distribution (left) where there are no edges, the density estimate appears to be a reasonable approximation of the known Gaussian distribution. In comparison, though, the truncated Gaussian distribution (right) fails to represent the hard cut-off at x = 0, instead \"leaking\" below zero with non-zero density despite the known closed boundary.\n\nClosed boundaries are common among many probability distributions,[bounded] and therefore the need to estimate a density corresponding to a (semi-)bounded distribution arises often. This package provides a density estimator that uses any provided boundary conditions to account for edge boundary effects, reproducing a more faithful representation of the underlying distribution.\n\n[bounded]: For example, see the list of distributions with bounded and semi-infinite support on Wikipedia.\n\nRepeating the density estimation on the Gaussian and truncated Gaussian distributions shown above instead with this package's kde method:\n\nimport KernelDensityEstimation as KDE\n\nkde_gauss = KDE.kde(rv_gauss)\nkde_trunc = KDE.kde(rv_trunc, lo = 0.0, boundary = :closedleft)\nnothing  # hide\n\n(Image: )\nKernel density estimates using the same data as Figure 1 but now processed with this package, including additional information about the truncated boundary (right).\n\n\n\ndetails: Plotting Code\nfig = Figure(size = (800, 400))\n\n# Gaussian distribution & KDE\nref = (x, exp_gauss)\nkd = (kde_gauss...,)\ndraw_KD(fig[1, 1], rv_gauss, ref, kd, \"Gaussian\")\n\n# Truncation Gaussian distribution & KDE\nref = (x, exp_trunc)\nkd = (kde_trunc...,)\ndraw_KD(fig[1, 2], rv_trunc, ref, kd, \"Truncated Gaussian\")\n\nLabel(fig[0, :], \"KernelDensityEstimation.jl\", font = :bold, fontsize = 20)\n\nsave(\"example_kerneldensityestimation.svg\", fig)  # hide\nnothing  # hide\n\nMost obviously, the truncated distribution retains its closed boundary condition at x = 0 and does not suffer from the leakage and suppression of the peak that occurs with the KernelDensity estimator. Furthermore, both density curves are smoother due to use of higher-order estimators which simultaneously permit using [relatively] wider bandwidth kernels while retaining the shapes of peaks (and non-flat slopes at closed boundaries).","category":"section"},{"location":"extensions/#Package-Extensions","page":"Package Extensions","title":"Package Extensions","text":"Pages = [\"extensions.md\"]\nDepth = 2:2\n\nimportant: Important\nThis section describes features that are only available when using Julia v1.9 or newer.","category":"section"},{"location":"extensions/#ext-distributions","page":"Package Extensions","title":"Distributions.jl","text":"A univariate distribution from Distributions.jl can be used as a value for the bounds argument of kde, wherein the boundary conditions of the distribution will be used to automatically set appropriate values of lo, hi, and boundary.\n\nFor example, generating a density estimate for a non-negative parameter in a Markov chain Monte Carlo (MCMC) chain is often paired with a similarly non-negative prior. Instead of needing to explicitly determine and pass through the correct combination of lower and upper bounds and their boundary conditions, the prior distribution can be used instead.\n\nusing KernelDensityEstimation\nusing Distributions\nusing Random  # hide\nRandom.seed!(1234)  # hide\n\n# a non-negative constraint on a prior\nprior = truncated(Normal(0.0, 1.0), lower = 0.0)\n# proxy for an MCMC chain\nchain = rand(prior, 200)\n\n# prior-based boundary information on left is same as explicit options on right\nkde(chain; bounds = prior) == kde(chain; lo = 0.0, boundary = :closedleft)","category":"section"},{"location":"extensions/#ext-makie","page":"Package Extensions","title":"Makie.jl","text":"Plotting the UnivariateKDE object is natively supported within the Makie.jl system of packages. The density estimate is converted via the PointsBased trait and defaults to a line plot.\n\nPlotting via stairs is a special case, which correctly offsets the bin centers to the trailing bin edge (compatible with the default step = :pre behavior) and adds points to close the histogram with the x-axis.\n\nusing KernelDensityEstimation: kde, LinearBinning\nusing Random  # hide\nRandom.seed!(100)  # hide\n\n# 500 samples from a Chisq(Î½=4) distribution\nrv = dropdims(sum(abs2, randn(4, 500), dims=1), dims=1)\nnothing  # hide\n\nusing CairoMakie\n\nK = kde(rv; lo = 0.0, boundary = :closedleft)\nH = kde(rv; lo = 0.0, boundary = :closedleft,\n            bwratio = 1.0, method = LinearBinning())\n\nfig = Figure(size=(800, 300))\nax1 = Axis(fig[1, 1], title=\"stairs\", ylabel = \"density\", xlabel = \"value\")\nax2 = Axis(fig[1, 2], title=\"lines\")\nlinkaxes!(ax1, ax2)\nhideydecorations!(ax2, grid = false, ticks = false)\n\nstairs!(ax1, H)\nlines!(ax2, K)\n\nsave(\"ext_makie.svg\", current_figure())  # hide\nnothing  # hide\n\n(Image: )\n\n\n","category":"section"},{"location":"extensions/#ext-plots","page":"Package Extensions","title":"Plots.jl","text":"Plotting the UnivariateKDE object is natively supported within the Plots.jl ecosystem of backends by defining a plot recipe for RecipesBase.jl.\n\nThe density estimate is interpreted by default as a :line series type with xlabel \"value\" and ylabel \"density\".\n\nusing KernelDensityEstimation: kde, LinearBinning\nusing Random  # hide\nRandom.seed!(100)  # hide\n\n# 500 samples from a Chisq(Î½=4) distribution\nrv = dropdims(sum(abs2, randn(4, 500), dims=1), dims=1)\nnothing  # hide\n\nusing Plots\n\nK = kde(rv; lo = 0.0, boundary = :closedleft)\nH = kde(rv; lo = 0.0, boundary = :closedleft,\n            bwratio = 1.0, method = LinearBinning())\n\nplot(\n    plot(H, title = \"stairs\", seriestype = :stepmid),\n    plot(K, title = \"lines\", ylabel = nothing),\n    layout = (1, 2), size = (800, 300),\n    leftmargin = (2.5, :mm), bottommargin = (3.0, :mm),\n    link = :all, legend = false\n)\n\nwithenv(() -> savefig(\"ext_plots.svg\"), \"GKSwstype\" => \"nul\");  # hide\ncloseall();  # hide\nnothing  # hide\n\n(Image: )\n\n\n","category":"section"},{"location":"extensions/#ext-unicodeplots","page":"Package Extensions","title":"UnicodePlots.jl","text":"For quick, approximate visualization of a density within the terminal, an extension is provided for the UnicodePlots.jl package and extends the lineplot (and lineplot!) methods.\n\nusing KernelDensityEstimation\nusing Random  # hide\nRandom.seed!(100)  # hide\n\n# 500 samples from a Chisq(Î½=4) distribution\nrv = dropdims(sum(abs2, randn(4, 500), dims=1), dims=1)\nnothing  # hide\n\nusing UnicodePlots\n\nK = kde(rv; lo = 0.0, boundary = :closedleft)\nlineplot(K)","category":"section"}]
}
